\section{Overview of active learning methods}
\label{sec:al:methods}

In this section, we present algorithms for specific active learning methods and 
a reference to its implementation in the appendix. The 
\texttt{activelearning} package by \texttt{ramhiser}~\cite{ramhiser2015} has 
adapted much of the methods reviewed in Settles' paper~\cite{settles2010}. As 
such, most of the implementation code has been adapted and reworked (with 
substantial components written from scratch) from the 
\texttt{activelearning} package, which is too general for our purposes.

\subsection{Uncertainty sampling}
\label{sec:al:methods:uncertainty} 

In uncertainty sampling, the active learner selects the query $q$ that it is 
most uncertain on how to label; in other words, the algorithm queries the label 
that has the highest posterior probability~\cite{lewis1994}. With binary 
classification as is the case with the VS (either ``visually correlated'' or 
``not visually correlated''), this reduces to the case of querying the instance 
whose posterior probability of being ``visually correlated'' is closest to 
0.5~\cite{lewis1994}. While the uncertainty sampling algorithm presented in 
Algorithm~\ref{alg:al:methods:uncertainty} follows this 
methodology and may subsequently only be used with classification models that 
are able to encompass posterior probability computations, there has been much 
work done to expand uncertainty sampling to non-probabilistic classifiers such 
as decision trees and nearest-neighbor~\cite{settles2010}.

Uncertainty sampling is simple but not without problem. Given a multi-label 
classification problem (labels $k > 2$), uncertainty sampling only 
considers the information about the most probable label $i$ and ignores the 
other possible labels $j \in \{1,...,k\}\backslash i$. ``Margin sampling'' and 
``entropy'' are variants that try to solve for these problems, but 
both reduce to the scheme of querying from the sample with a posterior 
probability closest to 0.5 when $k=2$ (binary 
classification)~\cite{settles2010}.

We have developed an algorithm for uncertainty sampling based on the literature 
review (see Appendix~\ref{sec:appendicies:al:uncertainty} for code):

\tablespacing
\begin{algorithm}[H]
	\caption{Uncertainty sampling (as described by 
	Settles~\cite{settles2010})}\label{alg:al:methods:uncertainty}
	\begin{algorithmic}[1]
		\Procedure{}{$X$ is a $n\times d$ matrix of $d$ observations of all $n$ 
		variables, $y$ is an $n$-length vector of labels for each variable in 
		$X$ ($y_i$=N/A when $X_{i,}$ has no label)}
		\State $\textit{tout} \gets 
		\text{train}(X^{\text{labeled}},y^{\text{labeled}},\text{classification 
		model})$
		\State $p \gets 
		\text{predict}(\textit{tout},X^{\text{unlabeled}},
		\text{posterior prob. = TRUE})$
		\State \textbf{loop from} $i=1$ \textbf{to} len($p$):
		\State \indent $p_i \gets |p_i-0.5|$
%		\If {$\textit{string}(i) = \textit{path}(j)$}
%		\State $j \gets j-1$.
%		\State $i \gets i-1$.
%		\State \textbf{goto} \emph{loop}.
%		\State \textbf{close};
%		\EndIf
		\State \textbf{return} where($p==\min{(p)}$)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\bodyspacing

\noindent By searching for the most ``uncertain'' point, uncertainty sampling 
is able to further refine the classifier as the oracle must label the point 
either ``visually correlated'' or ``not visually correlated''. This can be 
viewed as a search within the hypothesis space $\mathcal{H}$ that contains 
multiple classifiers, instances of a single classification model.








\subsection{Query by committee}
\label{sec:al:methods:qbc}

Query by committee is another case of efficient search through the hypothesis 
space. In query by committee, a ``committee'' of classification models are 
trained on the  current labeled instances.  Each model represents a competing 
hypothesis, and its prediction for an unlabeled query candidate $q$ is a 
``vote'' of weight one on $q$; the most disagreeable candidate is then 
queried~\cite{settles2010}. Settles reviews various methods for initial 
committee selection by the active learner (for both probabilistic and 
non-probabilistic models), though our implementation of QBC 
(Appendix~\ref{sec:appendicies:al:qbc}) allows the user to specify the 
committee members~\cite{settles2010}. 

The basic algorithm is as follows:

\tablespacing
\begin{algorithm}[H]
	\caption{Query by committee (as described by 
	Settles~\cite{settles2010})}\label{alg:al:methods:qbc1}
	\begin{algorithmic}[1]
		\Procedure{}{$X$ is a $n\times d$ matrix of $d$ observations 
		of all $n$ variables, $y$ is an $n$-length vector of labels for each 
		variable in $X$ ($y_i$=N/A when $X_{i,}$ has no label). Let $C$ be the 
		vector of all committee members i.e. classification models}
		\State \textbf{loop from} $i=1$ \textbf{to} len($C$):
		\State \indent $\textit{tout}_{i} \gets 
		\text{train}(X^{\text{labeled}},y^{\text{labeled}},C_i)$
		\State \indent $p_{i,} \gets 
		\text{predict}(\textit{tout}_i,X^{\text{unlabeled}})$
		\State $d \gets \text{disagreement}(p)$
		\State \textbf{return} where($d==\max{(d)}$)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\bodyspacing

\noindent While it is simpler to maintain the same committee 
throughout, it would be more informative to prune the committee as the 
algorithm proceeds; it may simply be the case that a model is ill-suited for 
the problem at hand and consistently returns predictions that skew the voting 
procedure. Subsequently, a model is removed from the committee if its 
predictions are consistently out-of-line with whatever the ``true'' label of 
$q_t$ (the next query point decided and then queried at time $t$) turns out to 
be. It is important to note that the ``true'' label is not known until 
\textit{after} the algorithm has returned the index of $q_t$. In the VS, this 
index corresponds to the next pairwise scatter plot to show the 
user and query. Subsequently, the committee from $t$ 
is pruned at time $t+1$ at the start of the algorithm using the label retrieved 
from time $t$. The pruning function should only 
be run after a good number of iterations (where each iteration results in a 
query) have passed to allow the error ratios to converge (Otherwise, there is 
no room for learning). In our implementation, we begin using the pruning 
algorithm after $iter/2$ queries where $iter$ is the query budget (the maximum 
number of queries allowed).
Furthermore, the query by committee methodology described by Settles only uses 
the committee to select $q$; thus, the committee is independent of the final 
classification model (in the VS, this is a random forest) once the budget has 
been used up~\cite{settles2010}. However, there may be merit in maintaining the 
final pruned committee as its own classification model after the budget has 
been used up. As a classification model, the pruned committee may label 
unlabeled instances via majority vote. Further details on implementation and a 
performance comparison may be found in the simulation study 
(Section~\ref{sec:al:simulations}). The revised algorithm is as follows (see 
Appendix~\ref{sec:appendicies:al:qbc} for code):

\tablespacing
\begin{algorithm}[H]
	\caption{Query by committee (revised framework)}\label{alg:al:methods:qbc2}
	\begin{algorithmic}[1]
		\Procedure{}{$X$ is a $n\times d$ matrix of $d$ observations of all $n$ 
		variables, $y$ is an $n$-length vector of labels for each variable in 
		$X$ ($y_i$=N/A when $X_{i,}$ has no label). Let $C$ be the vector of 
		all committee members i.e. classification models, $E$ be the average 
		error ratio of the respective 
		committee members (initialized to 0), $0<\epsilon<1$ be some threshold 
		for the error ratio, \textit{iter} be the total active learning 
		budget, and $t$ be the current iteration of QBC starting 
		at $t=1$}
		
		\Function{QBC}{}
		\State \textbf{loop from} $i=1$ \textbf{to} len($C$):
		\State \indent $\textit{tout}_i \gets 
		\text{train}(X^{\text{labeled}},y^{\text{labeled}},C_i)$
		\State \indent $p_{i,} \gets 
		\text{predict}(\textit{tout}_i,X^{\text{unlabeled}})$
		\State $d \gets \text{disagreement}(p)$
		\State \textbf{return} $j =$ where($d==\max{(d)}$)
		\EndFunction
		
		\Function{Oracle}{}
		\State \textbf{return} $l$, the label of $X_{j,}$
		\EndFunction
		
		\State $y_j \gets l$
		
		\Function{Prune }{When iteration $i \in [1,iter] > iter/2$, run 
		\textsc{Prune}}	
		\State $prune = [\ ]$ (empty vector)	
		\State \textbf{loop from} $i=1$ \textbf{to} len($C$):
		\State \indent \textbf{If} ($p_{i,j}==y_j$) \textbf{then} $iv = 0$ 
		\textbf{Else} $iv = 1$
		\State \indent $E_i = E_i + \frac{iv - E_i}{t}$
		\State \indent \textbf{If} $E_i>\epsilon$ \textbf{then} 
		$prune.\text{append}(i)$
		\State $t++$
		\State \textbf{return} $prune$
		\EndFunction
		
		\State \textbf{loop from} $i=1$ \textbf{to} len($prune$):
		\State \indent Delete $E_{prune_i}$, $C_{prune_i}$, $p_{prune_i,}$, 
		and $\textit{tout}_{prune_i}$
		
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\bodyspacing

Note that both Algorithm~\ref{alg:al:methods:qbc1} 
and~\ref{alg:al:methods:qbc2} contain a generic disagreement function, which 
measures the disagreement among the committee members. Let $Y$ be the set of 
all possible labels. There are two main methods of measuring disagreement 
described by Settles~\cite{settles2010}:

\tablespacing
\begin{enumerate}
	\item \textbf{Vote entropy}: The disagreement for query candidate $q$ is 
	computed
	$$d_{\text{VE}}(q) =-\sum\limits_{y \in Y} 
	\frac{V(y|q)}{\text{len}(C)} \log \frac{V(y|q)}{\text{len}(C)}$$
	\noindent where $0 \leq V(y|q) \leq \text{len}(C)$ is the number of votes 
	that label $y$ received for query candidate $q$. 
	The interested reader may refer to Dagan and 
	Engelson~\cite{dagan1995} for further details.
	
	\item \textbf{Kullback-Leibler divergence}: The disagreement for query 
	candidate $q$ is computed
	$$d_{\text{KL}}(q) = \frac{1}{\text{len}(C)} \sum\limits_{i =
	1}^{\text{len}(C)} \sum\limits_{y \in Y} 
	\text{P}_{C_i}(y|q) \log \frac{\text{P}_{C_i}(y|q)}
	{\text{P}_{C}(y|q)}$$
	\noindent Recall that $C$ is the vector of committee members. We can 
	interpret $\text{P}_{C}(y|q) = \frac{1}{\text{len}(C)} 
	\sum\limits_{k=1}^{\text{len}(C)} \text{P}_{C_k}(y|q)$ as the probability 
	that label $y$ will have the most votes for $q$, and 
	$\text{P}_{C_k}(y|q)$ as the probability that committee member $k$ votes 
	$y$ for $q$. The interested reader may refer to McCallum and 
	Nigam~\cite{mccallum1998} for further details.
\end{enumerate}
\bodyspacing

\noindent An implementation of vote entropy disagreement can be found in 
Appendix~\ref{sec:appendicies:al:entropy}. This function was imported from the 
\texttt{activelearning} package developed by 
\texttt{ramhiser}~\cite{ramhiser2015} and simply calls the \texttt{entropy} 
package in \texttt{R}.









\subsection{Query by bagging}
\label{sec:al:methods:bagging}

Query by bagging and boosting was proposed
as a way to improve the performance of a single classifier by forming a 
committee of classifiers trained on random (weighted, in the case of Boosting) 
subsets of the labeled data~\cite{abe1998}. 
Bagging uniformly samples $k$ subsets from the labeled data to form a committee 
of $k$ different classifiers, which are trained with the same classification 
model (e.g. random forest)~\cite{abe1998}. The unlabeled data with the most 
disagreement among the committee members is then selected as the next oracle 
query (see Section~\ref{sec:al:methods:qbc} for details on disagreement 
measures). The algorithm is as follows (see 
Appendix~\ref{sec:appendicies:al:bagging} for code):

\tablespacing
\begin{algorithm}[H]
	\caption{Query by bagging (as described by Abe and 
	Mamitsuka~\cite{abe1998})}\label{alg:al:methods:bagging}
	\begin{algorithmic}[1]
		\Procedure{}{$X$ is a $n\times d$ matrix of $d$ observations 
			of all $n$ variables, $y$ is an $n$-length vector of labels for 
			each variable in $X$ ($y_i$=N/A when $X_{i,}$ has no label). 
			$num\_class$ is the desired number of committee members (subsets to 
			sample). Let $r \in (0,1)$ such that 
			$r*\text{len}(X^{\text{labeled}})$ is the number of points to 
			randomly sample for each subset of the labeled set.}
		\State \textbf{loop from} $i=1$ \textbf{to} $num\_class$:
		\State \indent $idx \gets \text{unif\_sample}(\text{labeled}, 
		r*\text{len}(X^{\text{labeled}}) )$
		\State \indent $\textit{tout}_{i} \gets 
		\text{train}(X_{idx, },y_{idx},\text{classification model})$
		\State \indent $p_{i,} \gets 
		\text{predict}(\textit{tout}_i,X^{\text{unlabeled}})$
		\State $d \gets \text{disagreement}(p)$
		\State \textbf{return} where($d==\max{(d)}$)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\bodyspacing

\noindent Since each iteration of the query selection is a new process of 
random committee training, there is no starting committee. Instead, what the 
user specifies is the classification model that each subset will be trained on. 
Subsequently, there is no need to (1) maintain error ratios to prune a starting 
committee or to (2) use majority vote with a pruned committee for the final 
fitted model over a random forest, which is the classification model for stage 
2 (Section~\ref{sec:visualizer:plotgeneration}).










\subsection{Min-max clustering}
\label{sec:al:methods:clustering}

The goal of the Min-Max Approach is to query points that are far from each 
other~\cite{vu2010}. This can be done by selecting a query that maximizes the 
minimal distance of each unlabeled query from the labeled set~\cite{vu2010}. 
In other words, 
$$\max\limits_{q \in X^{\text{unlabeled}}} \bigg( \min\limits_{k \in 
X^{\text{labeled}}} \bigg( \text{distance}(q,k) \bigg) \bigg)$$

Given natural clustering in the data, the active learner would be able to 
sample from each cluster with this methodology. If each cluster is fairly 
homogeneous, the final fitted classifier will be a good representation of the 
data set's ``true'' labels (Section~\ref{sec:al:litreview}). As such, min-max 
clustering is able to exploit the clustering structure of data but 
may perform more poorly (converge more slowly i.e. require more queries to get 
to a reasonable error level) in data sets that do not naturally form clusters. 
Naturally, there are two important considerations:

\tablespacing
\begin{itemize}
	\item \textbf{Initialization:} How does the active learner find the 
	clusters in the first place? Selecting points near the centers of the 
	clusters, which are the densest regions, allows the algorithm 
	to converge faster~\cite{vu2010}.
	
	\item \textbf{Query selection:} How does the active learner quantify the 
	distance between data points? Euclidean distance is a common metric of the 
	straight-line distance between points in the Euclidean space.
\end{itemize}
\bodyspacing

Vu \textit{et al.} proposed the creation of a $k$-nearest neighbors graph 
to measure the local density of each data point; the most dense points may be 
selected to initialize the active learner~\cite{vu2010}. Since the VS will be 
initialized randomly for reasons described in 
Section~\ref{sec:visualizer:al:initialization}, we do not concern ourselves too 
much with min-max clustering initialization; 
the interested reader may refer to~\cite{vu2010} for 
more of the mathematical details and a demonstration of viability. 
Instead, we present the simple algorithm for the actual active learning 
(querying) process (see Appendix~\ref{sec:appendicies:al:clustering} for code):

\tablespacing
\begin{algorithm}[H]
	\caption{Min-max clustering (as described by 
		Vu \textit{et al.}~\cite{vu2010})}\label{alg:al:methods:clustering}
	\begin{algorithmic}[1]
		\Procedure{}{$X$ is a $n\times d$ matrix of $d$ observations of all $n$ 
			variables, $y$ is an $n$-length vector of labels for each variable 
			in $X$ ($y_i$=N/A when $X_{i,}$ has no label)}
		\State \textbf{loop from} $i=1$ \textbf{to} len($y^{\text{unlabeled}}$):
		\State \indent $\min \gets \infty$
		\State \indent \textbf{loop from} $j=1$ \textbf{to} 
		len($y^{\text{labeled}}$):
		\State \indent \indent  $d \gets 
		\text{distance}(X_{i,}^{\text{unlabeled}},X_{j,}^{\text{labeled}})$
		\State \indent \indent \textbf{If} $(\min >d): \min \gets d$
		\State \indent $q_i \gets \text{min}$
		\State \textbf{return} where($q==\max{(q)}$)
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\bodyspacing