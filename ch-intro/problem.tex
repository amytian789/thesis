\section{Problem statement}
\label{sec:intro:problem}

Often in modern multivariate analyses, data analysts rely solely on statistical estimators to explore the data. However, since each estimator inherently performs well or poorly under different settings, data analysts are unable to differentiate between the properties intrinsic to the dataset and the spurious properties the estimators added. Furthermore, the rapid increase in computing resources has led to the proliferation of high-dimensional datasets, which are more tedious to properly visualize. The problem is to find a way to visualize data to improve decision making and verify numerical models. To solve this, we develop a system that allow future analysts to visually confirm models determined through numerical methods. This enables data analysts to gain intuition on patterns in the data and recognize problems with various estimators. Furthermore, this tool increases standardization, and subsequently accountability, within data analysis. Our primary application focus is on graphical models and their financial applications. However, the visualization framework discussed in this work can be applied to any data analysis.

\subsection{Motivation}
\label{sec:intro:problem:motivation}

Despite the fact that more than 2.5 quintillion bytes of data is produced daily to be analyzed, there is still a lack of accountability and consistency in modern data analyses. ``Statistical thinking and methodology'' has become the framework for disciplines such as education, agriculture, economics, biology, medicine, astronomy, geology, and physics~\cite{efron1986}, but it is unclear if data analysts agree on the procedures to analyze data. Little, a Professor of Biostatics at the University of Michigan, notes that ``developing good statistical solutions to real applied problems, based on good science rather than `cookbookery,' is far from easy''~\cite{little2013}. This lack of agreement and ``cookbook'' mentality of data analyses has far-reaching consequences. It is simple to run the data through a list of many estimators and cherry-pick the most ``interesting'' result. Similarly, an analyst can remove undesirable data points without justification or unknowingly fit egregiously incorrect models. Regardless of whether all these situations are performed maliciously or with good intentions, the art of data analysis is unclear without standards. The lack of clear-cut guidelines makes it difficult for analysts to discern the ``truth'' from the data and avoid the aforementioned pitfalls while simultaneously making it difficult for consumers of the resulting analyses to evaluate how trustworthy it is. 

What is striking here is the lack of progress on this particular subject beyond the development of numerical methods. In computer science, a framework for ``clean code'' has been extensively documented and is the accepted industry standard for writing, interacting with, and thinking about code. But currently, in data analysis, analysts blindly depend on estimators and hypothesis tests to explore the data and have no justification of their analyses aside from asymptotic, mathematical guarantees. Hence, no such standards of ``clean analysis'' currently exist in data science despite the role its importance in financial decisions, judicial evidence, government policy, and scientific discovery. In this work, we tackle the problem by providing data analysts with a sophisticated visualization system to explore the data and numerical model in a different way. This way, future analysts can combine visual feedback with the numerical feedback from estimators to make better decisions during data analysis and provide clear justification of their decisions.