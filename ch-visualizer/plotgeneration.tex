\section{Automated plot generation (Stage 2)}
\label{sec:visualizer:plotgeneration}

\subsection{Decision tree classification of user interests}
\label{sec:visualizer:plotgeneration:tree}

Given initialization and actively selected labeled data, what classification 
models can the visualization system use to create a final fitted model of user 
interests? A \textit{decision tree} is composed of nodes (which correspond to 
classification labels) and branches (which correspond to decision boundaries). 
A tree is constructed at each node by sampling all $M$ possible vertical and 
horizontal splits in the sample space and selecting the split which minimizes 
the \textit{Gini criterion}. A mapping of the sample space to a tree is shown 
in Figure~\ref{fig:visualizer:plotgeneration:tree}. The Gini criterion measures 
the homogeneity of the nodes in each side of the proposed split and is computed 
as follows (for a vertical split)
$$G = N_{left} \sum\limits_{k=1}^{K} p_{k,left}(1-p_{k,left}) + N_{right} 
\sum\limits_{k=1}^{K}  p_{k,right}(1-p_{k,right})$$

\noindent where $N_s$ is the number of nodes in side $s$ of a split ($s = 
\{left,right\}$ in a vertical split, and $s = \{top,bottom\}$ in a horizontal 
split) and $p_{k,s}$ is the fraction of class label $k$ on side $s$ of the 
split. Decision trees are a more sophisticated classification method than 
simple linear regression and retains interpretability. The root node is, 
naturally, most important as it corresponds to a split that optimizes the Gini 
criterion while the terminal nodes can be thought of as the data's homogeneous 
clusters. However, decision trees are also unstable; perturbing a single 
data point may change the entire tree. Furthermore, due to the nature of 
vertical and horizontal splitting, 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=1\linewidth]{ch-visualizer/figures/decisiontree}
		\caption[Mapping the sample space to a decision tree.]{Mapping the 
		sample space (left) to a decision tree (right). Images from 
		Cutler~\cite{cutler2010}.}
		\label{fig:visualizer:plotgeneration:tree}
	\end{center}
\end{figure}

A \textit{random forest} provides solutions to the problems that a single 
decision tree faces. In a random forest, a forest is composed of many trees 
``grown'' from random partitions of the labeled set (the training set). 
Furthermore, each decision tree is constructed by finding the best split among 
$m \in M$ splits. This allows each tree to specialize on a subset of the data, 
creating a more informative aggregate. A simplistic example of a forest with 2 
trees may be found in Figure~\ref{fig:visualizer:al:tree}.
Each tree in the forest has a vote of weight one for each unlabeled data, and 
the forest is aggregated by majority vote, which makes the resulting decision 
boundaries more stable (less variant) on average. On the other hand, a forest 
with many trees is difficult to visualize. However, there are methods to 
simplify a forest into a single tree for the purposes of visualization (rather 
than for usage in classification, as the issue with stability would then 
remain). One such method for single tree approximate is presented by Zhou and 
Hooker~\cite{zhou2016}. Utilization of such a methodology aids in user 
interpretability of the system output while maintaining the robustness of a 
forest. As such, the VS sets the choice of classification model to random 
forest by default but allows the user to specify their preferred classification 
model if they wish to do so.

\subsection{User interaction with active learning output}
\label{sec:visualizer:plotgeneration:user}

The system has now learned which of the unlabeled plots 
may be of interest to the user. The final learned classifier is used to fit the 
rest of the $d \choose 2$ unlabeled scatterplots, and a visual graph $G=(V,E)$ 
may be built from these labels with the following heuristic where $i,j \in 
\{1,...,d\}$:

\begin{algorithm}
	If label($i,j)>0$ (i.e. ``interesting'' instead of ``uninteresting''), draw 
	edge $E_{i,j}$
\end{algorithm}

Furthermore, the VS may provide a visualization of the resulting classifier 
itself such as the decision tree itself (as discussed earlier). The active 
learning output may also be visualized as a heat map. A 
classic heat map represents each pair of variables as colors from a bivariate 
spectrum. Heat maps are difficult to interpret as it is one-dimensional; each 
end of the color spectrum represents minimum and maximum values respectively. 
It is unclear what the max or the min is as it depends on the domain of the 
whatever the heat map is plotting; as such, the minimum may not necessarily be 
negative while the maximum may not necessarily be positive. Furthermore, the 
subtle variations in hue between colors make it difficult to compare relative 
ranking among different pairs for colors that are similar. Buja \textit{et al.} 
propose an alternate, clearer method of visualizing the heat map, termed the 
``association navigator''~\cite{buja2016}. The association navigator is 
two-dimensional: the size of each color corresponds to its value while the 
color 
represents a positive or negative value~\cite{buja2016}. As such, the 
association navigator only utilizes two colors rather than a spectrum of 
colors, making it simple to distinguish between the two. By simplifying the 
color scheme and adding the dimension of size, the association navigator makes 
it much easier to interpret and compare different pairs of data at a glance. 
The stark difference between a traditional heatmap and the association 
navigator when applied to the same dataset may be seen in 
Figure~\ref{fig:visualizer:heatmap}. The VS provides both options, allowing the 
analyst to select whichever is easier to for them to interpret. 
With these visualizations of the active learning output, the user should be 
able to understand his/her own interests. 

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=1\linewidth]{ch-visualizer/figures/heatmap}
		\caption[Heatmap versus association navigator]{Heatmap versus 
		association navigator example.}
		\label{fig:visualizer:heatmap}
	\end{center}
\end{figure}

\subsection{System output}
\label{sec:visualizer:plotgeneration:output}

There are three options at the end of stage 2. Two of them are concrete outputs 
that may be easily used in an analyst's final report, and the third is a 
refinement of the active learning component in stage 1.

\tablespacing
\begin{itemize}
	\item \textbf{Automatic plot generation:} The VS compiles a selection of 
	the most interesting and non-interesting plots along with their 
	associated transformation variables.
	\item \textbf{Graph comparison:} The VS accepts a numeric graph (For 
	example, a correlation graph $G^{\text{num}}$ generated with numerical 
	correlation 
	coefficients as described in Section~\ref{sec:intro:correlation}) and 
	measures the difference between the numeric graph $G^{\text{num}}$ and 
	visual graph $G$ (the active learning output). 
	Details on different graph comparison methods may be found in 
	Chapter~\ref{ch:gc}.
	\item \textbf{Line-up test:} In the event that the classifier is not a 
	satisfactory representation of the analyst's interests, the VS may utilize 
	line-up tests to help determine where to query from further. For more 
	details on this methodology, see Section~\ref{sec:futurework:lineup}.
\end{itemize}
\bodyspacing

With the focus on correlation graphs in this work, graph comparison is the most 
useful output from the visualization system. As such, it is one of our primary 
VS focuses. The next section provides a roadmap for the rest of this work, 
which goes into detail on two aspects of the VS. 