\chapter{Implementation\label{ch:implementation}}

\lstset{basicstyle=\ttfamily\footnotesize,xleftmargin=0cm,breaklines=true,language=R}

\section{Code for figure~\ref{fig:intro:meplot}, left}
\label{sec:appendicies:me1plot}
{\setstretch{1.0}
\begin{lstlisting}
#generate a reproducible dataset and scale to [0,1]
set.seed(10)
x <- seq(0, 1, length.out = 100)
y <- rnorm(100)
y <- (y-min(y))/(max(y)-min(y))

#sort the noise
y <- sort(y)
y <- y[c(seq(1,99,length.out=50), seq(100,2,length.out=50))]

#local swapping
for(i in 4:96){
	y[(i-3):(i+3)] <- y[sample((i-3):(i+3))]
}

idx <- sample(1:100)
x <- x[idx]; y <- y[idx]

#####################
#numerical feedback

## fit linear regression
fitlm <- lm(y ~ x)
anova(fitlm)

## see if any coefficients are significant
summary(fitlm)

## see if residuals are normally-distributed
shapiro.test(fitlm$residuals)

## correlation is not significantly different from zero
cor.test(x, y)

####################
#visual feedback
plot(x, y, pch = 16, cex = 2)
\end{lstlisting}
}


\section{Code for figure~\ref{fig:intro:meplot}, right}
\label{sec:appendicies:me2plot}
{\setstretch{1.0}
\begin{lstlisting}
## generate a reproducible dataset
set.seed(10)
n <- 50
x <- sort(rnorm(n))
sd.vec <- c(seq(1, 1.5, length.out = 50), seq(1.5, 1, length.out = 50))
y <- -x + 0.5*rnorm(n, sd = sd.vec)
y <- scale(y)

y[c(1,5,10)] <- min(y)
y[c(n-10, n-5, n)] <- max(y)

#####################
#numerical feedback

## fit linear regression
fitlm <- lm(y ~ x)
anova(fitlm)

## see if any coefficients are significant
summary(fitlm)

## see if residuals are normally-distributed
shapiro.test(fitlm$residuals) 

## correlation is not significantly different from zero
cor.test(x, y)

####################
#visual feedback
plot(x,y, pch = 16, cex = 2)
\end{lstlisting}
}


\section{Code for figure~\ref{fig:visualizer:cdf}}
\label{sec:appendicies:cdf}
{\setstretch{1.0}
\begin{lstlisting}
## generate the dataset
set.seed(10)
n <- 500
x <- rnorm(n)
y <- rnorm(n)

par(mfrow=c(1,2))
plot(x,y, pch = 16, cex = 2)
## apply the cdf
plot(pnorm(x),pnorm(y),pch = 16, cex = 2)
\end{lstlisting}
}

\section{Uncertainty sampling (Section \ref{sec:al:methods:uncertainty}) 
implementation}
\label{sec:appendicies:al:uncertainty}

Refer to Algorithm \ref{alg:al:methods:uncertainty}. 
{\setstretch{1.0}
\begin{lstlisting}
#' Uncertainty Sampling with bivariate labels
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param classifier the classifier name
#' @param ... additional parameters for the active learning method
#'
#' @return a vector of indices to query
#' @export

uncertainty_sample <- function(X, y, unlabel_index_c, classifier, ...){
	if (length(classifier) > 1 || missing(classifier) || is.null(classifier) || 
	is.na(classifier)) {
		stop("A single classifier is required for uncertainty sampling")
	}
	
	# Check that the classifier is compatible with uncertainty sampling
	c <- try(caret::modelLookup(classifier))
	if (!any(c$probModel)) {
		stop(classifier," must return posterior probabilities")
	}
	
	# Split x and y to retrieve labeled and unlabeled pairs
	unlabel_index <- which(is.na(y))
	x_lab <- X[-unlabel_index,]
	y_lab <- y[-unlabel_index]
	x_ulab <- X[unlabel_index_c,]
	
	tout <- caret::train(x_lab,y_lab,classifier)
	p <- as.matrix(stats::predict(tout, newdata=x_ulab, type="prob"))
	
	# Return corresponding X index of posterior closest to 0.5
	p <- apply(p, 1, function(x) abs(x[1]-0.5))
	unlabel_index_c[which(p == min(p))]
}
\end{lstlisting}
}

\section{Query by committee implementation}
\label{sec:appendicies:al:qbc}

Refer to Algorithm \ref{alg:al:methods:qbc2}. 
This implementation contains the functions for query selection and pruning. 
These functions are called by the main simulation engine for the QBC method. 
The main simulation engine acts as the entire algorithm in 
Section~\ref{sec:al:methods:qbc}. The simulation engine code may be found in 
Appendix~\ref{sec:appendicies:al:simulations}.
\subsection{Query selection}
{\setstretch{1.0}
\begin{lstlisting}
#' Query by Committee
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param committee the list of committee classifiers
#' @param dis is the disagreement measure between committee classifications
#' @param isMajority is if overall classifier Majority Vote or Random Forest
#' @param tout is a list of trained classifiers from Majority Vote computation 
#' @param ... additional parameters for the active learning method
#'
#' @return a vector of indices to query AND committee predictions
#' @export

qbc_sample <- function(X, y, unlabel_index_c, committee, dis = "vote_entropy", 
isMajority = FALSE, tout = NULL, ...) {
	if (missing(committee)||is.null(committee)) stop("A committee is required")
	if (isMajority & is.null(test)) {
		stop("Re-feed the majority vote return to the next QBC_sample call")
	}
	
	unlabel_index <- which(is.na(y))
	x_lab <- X[-unlabel_index,]
	y_lab <- y[-unlabel_index]
	x_ulab <- X[unlabel_index_c,]
	p <- vector("list",length(committee))
	
	if (!isMajority) {
		for (i in 1:length(committee)) {
			tout <- caret::train(x_lab,y_lab,committee[i])
			p[[i]] <- predict(tout, newdata=x_ulab)
		}
	} else {
		# Reuse the trained classifiers from the majority vote call
		for (i in 1:length(committee)) {
			p[[i]] <- predict(tout[[i]], newdata=x_ulab)
		}
	}

	# Compute disagreement (functions from the activelearning package)
	d <- switch(dis,
		vote_entropy=vote_entropy(p),
		post_entropy=post_entropy(p),
		kullback=kullback(p)
		)
	
	index <- unlabel_index_c[which(d == max(d))]
	if (length(index) > 1) index <- sample(index,1)
	# Gather each committee's prediction
	pre <- rep(0,length(committee))
	for (i in 1:length(committee)) {
		# Predict function returns a factor
		pre[i] <- 
		as.numeric(as.character(p[[i]][which(unlabel_index_c==index)]))
	}
	
	list(index, pre)
}
\end{lstlisting}
}



\subsection{Committee pruning}
{\setstretch{1.0}
\begin{lstlisting}
#' Query by Committee (Pruning function)
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param index is the classification of X[index,] which was queried
#' @param committee_pred is the list of committee predictions for index
#' @param k is the current iteration number that the AL_engine is on
#' @param pt is the pruning threshold (any error value above it is pruned)
#' @param err in (0 best,1 worst) is the committee's error-to-iteration ratio
#' @param is_prune is TRUE when pruning is desired, FALSE when not
#' @param ... additional parameters for the active learning method
#'
#' @return a list with updated error, indices to delete from the committee
#' @export
qbc_prune <- function(X, y, index, committee_pred, k, pt = 0.5, err, is_prune, 
...){
	if (missing(err) || is.null(err) || is.na(err)) {
		stop("Committee error ratio is required for QBC pruning")
	}
	prune <- vector() # Do not know how long prune will be until the end
	# Do not prune if committee size is 1 or it's the first round
	if (length(committee_pred) == 1 | k == 1) {
		list(err, prune)
	} else {
		# Update error value
		for (i in 1:length(committee_pred)) {
			if (committee_pred[i] == y[index]) iv <- 0 else iv <- 1
			err[i] <- err[i] + (iv - err[i])/k
			if (err[i] > pt & is_prune) {
				prune <- c(prune,i)
			}
		}
		list(err, prune)
	}
}
\end{lstlisting}
}

\section{Vote entropy implementation}
\label{sec:appendicies:al:entropy}

Refer to Section \ref{sec:al:methods:qbc}.
{\setstretch{1.0}
\begin{lstlisting}
#' Disagreement method (From activelearning package)
#' @importFrom itertools2 izip
#' @importFrom entropy entropy

vote_entropy <- function(x, type='class', entropy_method='ML'){
	it <- do.call(itertools2::izip, x)
	disagreement <- sapply(it, function(obs) {
		entropy::entropy(table(unlist(obs)), method=entropy_method)
	})
	disagreement
}
\end{lstlisting}
}



\section{Query by bagging implementation}
\label{sec:appendicies:al:bagging}

Refer to Algorithm \ref{alg:al:methods:bagging}. 
{\setstretch{1.0}
\begin{lstlisting}
#' Query by Bagging
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param classifier the name of a classification model
#' @param dis is the disagreement measure between committee classifications
#' @param num_class is the number of desired committee members
#' @param r in (0,1). r*(labeled set) = training set for each num_class round
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

qbb_sample <- function(X, y, unlabel_index_c, classifier, dis = "vote_entropy", 
num_class, r, ...){
	if(r<=0 || r>=1) stop("r must be in (0,1)")
	
	x_ulab <- X[unlabel_index_c,]
	
	# Randomly sample from the labeled set to create a classifier
	label_index <- which(!is.na(y))
	committee <- vector("list",num_class)
	for (i in 1:num_class) {
		idx <- sample(label_index,round(length(label_index)*r,0))
		committee[[i]] <- caret::train(X[idx,],y[idx],classifier)
	}
	
	# Utilize the resulting classifiers as a committee
	p <- vector("list",length(committee))
	for (i in 1:length(committee)) {
		p[[i]] <- stats::predict(committee[[i]], x_ulab)
	}
	
	# Compute disagreement (utilizing the functions from the activelearning 
	package)
	d <- switch(dis,
		vote_entropy=vote_entropy(p),
		post_entropy=post_entropy(p),
		kullback=kullback(p)
		)
	
	index <- unlabel_index_c[which(d == max(d))]
	if (length(index) > 1) index <- sample(index,1)
	index
}
\end{lstlisting}
}

\section{Min-max clustering implementation}
\label{sec:appendicies:al:clustering}

Refer to Algorithm \ref{alg:al:methods:clustering}. 
{\setstretch{1.0}
\begin{lstlisting}
#' Query by Min-Max Clustering
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param dis is the distance measure between data
#' @param ... additional parameters for the active learning method
#'
#' @return a vector of indices to query
#' @export

cluster_sample <- function(X, y, unlabel_index_c, dis = "euclidean", ...){
	label_index <- which(!is.na(y))
	x_lab <- X[label_index,]
	y_lab <- y[label_index]
	x_ulab <- X[unlabel_index_c,]
	y_ulab <- y[unlabel_index_c]
	
	# Select the point furthest from points that are already labeled
	q <- rep(0,length(y_ulab))
	for (i in 1:length(y_ulab)) {
		min <- Inf
		for (j in 1:length(y_lab)) {
			temp <- cs_distance(X[unlabel_index_c[i],],X[label_index[j],],dis)
			if (min > temp) min <- temp
		}
		q[i] <- min
	}
	unlabel_index_c[which(q==max(q))]
}

# General Support Function for Distance Computation
cs_distance <- function(a,b,dis = "euclidean"){
	d <- switch(dis,
		euclidean=cs_euclidean_distance(a,b)
		)
}

# Euclidean Distance Computation
cs_euclidean_distance <- function(a,b) {
	sqrt( sum( mapply( function(x,y) (x-y)^2, a, b)))
}
\end{lstlisting}
}

\section{Simulation implementation}
\label{sec:appendicies:al:simulations}

Refer to Section \ref{sec:al:simulations}.
\subsection{MNIST data}
\label{sec:appendicies:al:simulations:data}
{\setstretch{1.0}
\begin{lstlisting}
load_mnist <- function() {
	load_image_file <- function(filename) {
		ret = list()
		f = file(filename,'rb')
		readBin(f,'integer',n=1,size=4,endian='big')
		ret$n = readBin(f,'integer',n=1,size=4,endian='big')
		nrow = readBin(f,'integer',n=1,size=4,endian='big')
		ncol = readBin(f,'integer',n=1,size=4,endian='big')
		x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
		ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
		close(f)
		ret
	}
	load_label_file <- function(filename) {
		f = file(filename,'rb')
		readBin(f,'integer',n=1,size=4,endian='big')
		n = readBin(f,'integer',n=1,size=4,endian='big')
		y = readBin(f,'integer',n=n,size=1,signed=F)
		close(f)
		y
	}
	train <<- load_image_file('mnist/train-images-idx3-ubyte')	
	train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
}

show_digitsmall <- function(arr196, col=gray(12:1/12), ...) {
	image(matrix(arr196, nrow=14)[,14:1], col=col, ...)
}

compressImg <- function(full){
	compressFour <- function(j){
		pixelvec = rep(NA,4)
		pixelvec[1] = full[2*j-1+floor((j-1)/14)*28];
		pixelvec[2] = full[2*j+floor((j-1)/14)*28];
		pixelvec[3] = full[2*j-1+28+floor((j-1)/14)*28];
		pixelvec[4] = full[2*j+28+floor((j-1)/14)*28];
		return(mean(pixelvec))
	}
	
	compress = unlist(lapply(1:196,compressFour))
	return(compress)
}

plotTable <- function(numRow,numCol,vec.labels,mat.images){
	vec.uniq = unique(vec.labels)
	par(mfrow=c(numRow,numCol),pty="s",mar = c(0.1,0.1,0.1,0.1))
	for(i in 1:length(vec.uniq)){
		tmpidx = which(vec.labels==vec.uniq[i])
		for(j in 1:length(which(vec.labels==vec.uniq[i]))){
			show_digitsmall(mat.images[tmpidx[j],],asp=TRUE)
		}
	}
}
\end{lstlisting}
}

\subsection{Simulation engine}
\label{sec:appendicies:al:simulations:simengine}
{\setstretch{1,0}
\begin{lstlisting}
AL_engine <- function(X, y, y_unlabeled, al_method,
classifier_method, return_method, iter, n, ...) {
	
	stopifnot(nrow(X) == length(y), is.matrix(X), is.factor(y), 
	length(levels(y)) == 2)
	idx <- which(is.na(y_unlabeled))
	stopifnot(length(idx) > 0, all(y[-idx] == y_unlabeled[-idx]), length(y) == 
	length(y_unlabeled),
	is.factor(y_unlabeled))
	
	res <- rep(0,iter)
	
	### SET THE COMMITTEE HERE
	cm <- c("rf","nb","pls","svmRadialWeights")
	err<- rep(0,length(cm))
	
	for(i in 1:iter){
		# If QBC, the procedure is a little different....
		if (al_method == "qbc") {
			if (i != 1 & 
			as.character(substitute(classifier_method))=="qbc_majority") {
				# QBC Majority method re-trains committee after the oracle
				# Save computation time by passing those results to QBC algo
				next_sample <- active_learning(X=X, y=y_unlabeled, 
				almethod=al_method, n=n, committee = cm, 
				isMajority = TRUE, tout = tout, ...)
			} else {
			next_sample <- active_learning(X=X, y=y_unlabeled, 
			almethod=al_method, n=n, committee = cm, ...)
		}
		y_unlabeled[next_sample[[1]]] <- y[next_sample[[1]]]
		
		# Update error and prune committee
		### Comment out if no committee pruning is desired
		if (i > iter/2) {
			prune <- active_learning(X=X, y=y_unlabeled, almethod="qbc_prune", 
			n = n, index=next_sample[[1]],
			committee_pred=next_sample[[2]], k = i, err = err, is_prune = TRUE, 
			...)
			err <- prune[[1]]
			# check if there's stuff to prune
			if (length(prune[[2]] != 0)) {
				cm <- cm[-unlist(prune[[2]])]
				err <- err[-unlist(prune[[2]])]
			}
		}
		else {
			prune <- active_learning(X=X, y=y_unlabeled, almethod="qbc_prune", 
			n = n, index=next_sample[[1]],
			committee_pred=next_sample[[2]], k = i, err = err, is_prune = 
			FALSE, ...)
			err <- prune[[1]]
		}
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx], committee = cm)
		res[i] <- return_method(tout, X, y, committee = cm)
	}
	# Everything else (not QBC)
	else {
		next_sample <- active_learning(X, y_unlabeled, al_method, n, ...)
		y_unlabeled[next_sample] <- y[next_sample]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx])
		res[i] <- return_method(tout, X, y)
		}
	}
	res
}
\end{lstlisting}
}

\subsection{AL algorithm engine}
\label{sec:appendicies:al:simulations:alengine}
{\setstretch{1.0}
\begin{lstlisting}
#' Main active learning engine
#'
#' The missing labels in y are denoted by NA.
#' This method takes X as a matrix of all the data
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param almethod the AL method name
#' @param n is the number of unlabeled points to be "pooled"
#' @param ... additional parameters for the active learning method
#'
#' @return an index corresponding to the row of X to learn the label of next
#' @export

active_learning <- function(X, y, almethod = "us", n, ...){
	stopifnot(nrow(X) == length(y), is.matrix(X), any(is.na(y)),
	is.factor(y), length(levels(y)) == 2)
	
	if (n == 0) {
		unlabel_index_c <- which(is.na(y))
	} else unlabel_index_c <- sample(which(is.na(y)), n)
	
	switch(almethod,
		us=uncertainty_sample(X,y,unlabel_index_c, ...),
		rs=random_sample(unlabel_index_c, ns = 1, ...),
		qbc=qbc_sample(X,y,unlabel_index_c, ...),
		qbb=qbb_sample(X,y,unlabel_index_c,...),
		qbc_prune=qbc_prune(X=X, y=y, ...),
		cluster=cluster_sample(X,y,unlabel_index_c, ...)
		)
}
\end{lstlisting}
}

\subsection{Simulator (Main)}
\label{sec:appendicies:al:simulations:results}