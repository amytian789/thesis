\chapter{Implementation\label{ch:implementation}}

\lstset{basicstyle=\ttfamily\footnotesize,xleftmargin=0cm,breaklines=true,language=R}

\section{Code for Figure~\ref{fig:intro:meplot}, left}
\label{sec:appendicies:me1plot}
{\setstretch{1.0}
\begin{lstlisting}
# Generate a reproducible dataset and scale to [0,1]
set.seed(10)
x <- seq(0, 1, length.out = 100)
y <- rnorm(100)
y <- (y-min(y))/(max(y)-min(y))

# Sort the noise
y <- sort(y)
y <- y[c(seq(1,99,length.out=50), seq(100,2,length.out=50))]

# Local swapping
for(i in 4:96){
	y[(i-3):(i+3)] <- y[sample((i-3):(i+3))]
}

idx <- sample(1:100)
x <- x[idx]; y <- y[idx]

##################### Numerical feedback

## Fit linear regression
fitlm <- lm(y ~ x)
anova(fitlm)

## See if any coefficients are significant
summary(fitlm)

## See if residuals are normally-distributed
shapiro.test(fitlm$residuals)

## Correlation is not significantly different from zero
cor.test(x, y)

#################### Visual feedback
plot(x, y, pch = 16, cex = 2)
\end{lstlisting}
}


\section{Code for Figure~\ref{fig:intro:meplot}, right}
\label{sec:appendicies:me2plot}
{\setstretch{1.0}
\begin{lstlisting}
## Generate a reproducible dataset
set.seed(10)
n <- 50
x <- sort(rnorm(n))
sd.vec <- c(seq(1, 1.5, length.out = 50), seq(1.5, 1, length.out = 50))
y <- -x + 0.5*rnorm(n, sd = sd.vec)
y <- scale(y)

y[c(1,5,10)] <- min(y)
y[c(n-10, n-5, n)] <- max(y)

##################### Numerical feedback

## Fit linear regression
fitlm <- lm(y ~ x)
anova(fitlm)

## See if any coefficients are significant
summary(fitlm)

## See if residuals are normally-distributed
shapiro.test(fitlm$residuals) 

## Correlation is not significantly different from zero
cor.test(x, y)

#################### Visual feedback
plot(x,y, pch = 16, cex = 2)
\end{lstlisting}
}


\section{Code for Figure~\ref{fig:visualizer:cdf}}
\label{sec:appendicies:cdf}
{\setstretch{1.0}
\begin{lstlisting}
# Generate the dataset
set.seed(10)
n <- 500
x <- rnorm(n)
y <- rnorm(n)

# Plot data and apply CDF
par(mfrow=c(1,2))
plot(x,y, pch = 16, cex = 2)
plot(pnorm(x),pnorm(y),pch = 16, cex = 2)
\end{lstlisting}
}




\section{Uncertainty sampling}
\label{sec:appendicies:al:uncertainty}

Refer to Algorithm \ref{alg:al:methods:uncertainty}. 
{\setstretch{1.0}
\begin{lstlisting}

#' Uncertainty Sampling with bivariate labels
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param classifier the classifier name
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

uncertainty_sample <- function(X, y, unlabel_index_c, classifier,
	isR = FALSE, tout = NULL, ...){

	if (length(classifier) > 1 || missing(classifier) || is.null(classifier) || 
	is.na(classifier)) {
		stop("A single classifier is required for uncertainty sampling")
	}
	if (isR & is.null(tout)) {
		stop("Re-feed classifier_method return to next uncertainty_sample call")
	}	
	
	# Check that the classifier is compatible with uncertainty sampling
	c <- try(caret::modelLookup(classifier))
	if (!any(c$probModel)) {
		stop(classifier," must return posterior probabilities")
	}
	
	# Split X and y to retrieve labeled and unlabeled pairs
	unlabel_index <- which(is.na(y))
	x_lab <- X[-unlabel_index,]
	y_lab <- y[-unlabel_index]
	x_ulab <- X[unlabel_index_c,]
	
	if (!isR) {
		tout <- caret::train(x_lab,y_lab,classifier)
		p <- as.matrix(stats::predict(tout, newdata=x_ulab, type="prob"))
	} else {
		# Reuse the trained classifier from the classifier_method call
		# Of course, this only works since classifier = "rf", and the
		# classifier_method function also uses "rf"
		p <- as.matrix(stats::predict(tout, newdata=x_ulab, type="prob"))
	}
	
	# Return corresponding X index of posterior closest to 0.5
	p <- apply(p, 1, function(x) abs(x[1]-0.5))
	index <- unlabel_index_c[which(p == min(p))]
	if (length(index) > 1) index <- sample(index,1)
	index
}
\end{lstlisting}
}

\section{Query by committee}
\label{sec:appendicies:al:qbc}

Refer to Algorithm \ref{alg:al:methods:qbc2}. 
This implementation contains the functions for query selection and pruning. 
These functions are called by the main simulation engine (Appendix 
~\ref{sec:appendicies:al:simulations:simengine}) for the QBC method. 
The simulation engine is coded in such as way that it acts as the skeleton 
of the entire algorithm in Section~\ref{sec:al:methods:qbc}.

\subsection{Query selection}
{\setstretch{1.0}
\begin{lstlisting}

#' Query by Committee
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param committee the list of committee classifiers
#' @param dis is the disagreement measure between committee classifications
#' @param isMajority is if overall classifier Majority Vote or Random Forest
#' @param tout is a list of trained classifiers from Majority Vote computation 
#' @param ... additional parameters for the active learning method
#'
#' @return a list with: an index to query AND committee predictions
#' @export

qbc_sample <- function(X, y, unlabel_index_c, committee,
	dis = "vote_entropy", isMajority = FALSE, tout= NULL, ...){
	
	if (missing(committee)||is.null(committee)) stop("A committee is required")
	if (isMajority & is.null(tout)) {
		stop("Re-feed the majority vote return to the next QBC_sample call")
	}
	
	unlabel_index <- which(is.na(y))
	x_lab <- X[-unlabel_index,]
	y_lab <- y[-unlabel_index]
	x_ulab <- X[unlabel_index_c,]
	p <- vector("list",length(committee))
	
	if (!isMajority) {
		for (i in 1:length(committee)) {
			tout <- caret::train(x_lab,y_lab,committee[i])
			p[[i]] <- predict(tout, newdata=x_ulab)
		}
	} else {
		# Reuse the trained classifiers from the majority vote call
		for (i in 1:length(committee)) {
			p[[i]] <- predict(tout[[i]], newdata=x_ulab)
		}
	}

	# Compute disagreement (functions from the activelearning package)
	d <- switch(dis,
		vote_entropy=vote_entropy(p),
		post_entropy=post_entropy(p),
		kullback=kullback(p)
		)
	
	index <- unlabel_index_c[which(d == max(d))]
	if (length(index) > 1) index <- sample(index,1)
	# Gather each committee's prediction
	pre <- rep(0,length(committee))
	for (i in 1:length(committee)) {
		# Predict function returns a factor
		pre[i] <- 
		as.numeric(as.character(p[[i]][which(unlabel_index_c==index)]))
	}
	
	list(index, pre)
}
\end{lstlisting}
}



\subsection{Committee pruning}
{\setstretch{1.0}
\begin{lstlisting}

#' Query by Committee (committee pruning function)
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param index is the classification of X[index,] which was queried
#' @param committee_pred is the list of committee predictions for index
#' @param k is the current iteration number that the AL_engine is on
#' @param pt is the pruning threshold (any error value above it is pruned)
#' @param err in (0 best,1 worst) is the committee's error-to-iteration ratio
#' @param is_prune is TRUE when pruning is desired, FALSE when not
#' @param ... additional parameters for the active learning method
#'
#' @return a list with: updated error AND indices to delete from the committee
#' @export

qbc_prune <- function(X,y,index,committee_pred,k,pt = 0.5,err,is_prune,...){

	if (missing(err) || is.null(err) || is.na(err)) {
		stop("Committee error ratio is required for QBC pruning")
	}
	prune <- vector() # Do not know how long prune will be until the end
	# Do not prune if committee size is 1 or if it's the first round
	if (length(committee_pred) == 1 | k == 1) {
		list(err, prune)
	} else {
		# Update error value
		for (i in 1:length(committee_pred)) {
			if (committee_pred[i] == y[index]) iv <- 0 else iv <- 1
			err[i] <- err[i] + (iv - err[i])/k
			if (err[i] > pt & is_prune) {
				prune <- c(prune,i)
			}
		}
		list(err, prune)
	}
}
\end{lstlisting}
}

\section{Vote entropy}
\label{sec:appendicies:al:entropy}

Refer to Section \ref{sec:al:methods:qbc}.
{\setstretch{1.0}
\begin{lstlisting}

#' Disagreement method (from activelearning package)
#' @importFrom itertools2 izip
#' @importFrom entropy entropy

vote_entropy <- function(x, type='class', entropy_method='ML'){

	it <- do.call(itertools2::izip, x)
	disagreement <- sapply(it, function(obs) {
		entropy::entropy(table(unlist(obs)), method=entropy_method)
	})
	disagreement
}
\end{lstlisting}
}



\section{Query by bagging}
\label{sec:appendicies:al:bagging}

Refer to Algorithm \ref{alg:al:methods:bagging}. 
{\setstretch{1.0}
\begin{lstlisting}

#' Query by Bagging
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param classifier the name of a classification model
#' @param dis is the disagreement measure between committee classifications
#' @param num_class is the number of desired committee members
#' @param r in (0,1). r*(labeled set) = training set for each num_class round
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

qbb_sample <- function(X, y, unlabel_index_c, classifier, 
	dis = "vote_entropy", num_class, r, ...){

	if(r<=0 || r>=1) stop("r must be in (0,1)")
	
	x_ulab <- X[unlabel_index_c,]
	
	# Randomly sample from the labeled set to create a classifier
	label_index <- which(!is.na(y))
	committee <- vector("list",num_class)
	for (i in 1:num_class) {
		idx <- sample(label_index,round(length(label_index)*r,0))
		committee[[i]] <- caret::train(X[idx,],y[idx],classifier)
	}
	
	# Utilize the resulting classifiers as a committee
	p <- vector("list",length(committee))
	for (i in 1:length(committee)) {
		p[[i]] <- stats::predict(committee[[i]], x_ulab)
	}
	
	# Compute disagreement (functions from the activelearning package)
	d <- switch(dis,
		vote_entropy=vote_entropy(p),
		post_entropy=post_entropy(p),
		kullback=kullback(p)
		)
	
	index <- unlabel_index_c[which(d == max(d))]
	if (length(index) > 1) index <- sample(index,1)
	index
}
\end{lstlisting}
}

\section{Min-max clustering}
\label{sec:appendicies:al:clustering}

Refer to Algorithm \ref{alg:al:methods:clustering}. 
{\setstretch{1.0}
\begin{lstlisting}

#' Min-Max Clustering
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param dis is the distance measure between data
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

cluster_sample <- function(X,y,unlabel_index_c,dis="euclidean",...){

	label_index <- which(!is.na(y))
	x_lab <- X[label_index,]
	y_lab <- y[label_index]
	x_ulab <- X[unlabel_index_c,]
	y_ulab <- y[unlabel_index_c]
	
	# Select the point furthest from the labeled set
	q <- rep(0,length(y_ulab))
	for (i in 1:length(y_ulab)) {
		min <- Inf
		for (j in 1:length(y_lab)) {
			temp <- cs_distance(X[unlabel_index_c[i],],X[label_index[j],],dis)
			if (min > temp) min <- temp
		}
		q[i] <- min
	}
	index <- unlabel_index_c[which(q==max(q))]
	if (length(index) > 1) index <- sample(index,1)
	index
}

# Main distance engine
cs_distance <- function(a,b,dis = "euclidean"){
	d <- switch(dis,
		euclidean=cs_euclidean_distance(a,b)
		)
}

# Euclidean Distance
cs_euclidean_distance <- function(a,b) {
	sqrt( sum( mapply( function(x,y) (x-y)^2, a, b)))
}
\end{lstlisting}
}

\section{AL simulation study}
\label{sec:appendicies:al:simulations}

Refer to Section \ref{sec:al:simulations}.
\subsection{MNIST data}
\label{sec:appendicies:al:simulations:data}
{\setstretch{1.0}
\begin{lstlisting}

# Load the MNIST dataset
load_mnist <- function() {
	load_image_file <- function(filename) {
		ret = list()
		f = file(filename,'rb')
		readBin(f,'integer',n=1,size=4,endian='big')
		ret$n = readBin(f,'integer',n=1,size=4,endian='big')
		nrow = readBin(f,'integer',n=1,size=4,endian='big')
		ncol = readBin(f,'integer',n=1,size=4,endian='big')
		x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
		ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
		close(f)
		ret
	}
	load_label_file <- function(filename) {
		f = file(filename,'rb')
		readBin(f,'integer',n=1,size=4,endian='big')
		n = readBin(f,'integer',n=1,size=4,endian='big')
		y = readBin(f,'integer',n=n,size=1,signed=F)
		close(f)
		y
	}
	train <<- load_image_file('mnist/train-images-idx3-ubyte')	
	train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
}

# Plot a single digit
show_digitsmall <- function(arr196, col=gray(12:1/12), ...) {
	image(matrix(arr196, nrow=14)[,14:1], col=col, ...)
}

# Compress the MNIST dataset from 28x28 to 14x14
compressImg <- function(full){
	compressFour <- function(j){
		pixelvec = rep(NA,4)
		pixelvec[1] = full[2*j-1+floor((j-1)/14)*28];
		pixelvec[2] = full[2*j+floor((j-1)/14)*28];
		pixelvec[3] = full[2*j-1+28+floor((j-1)/14)*28];
		pixelvec[4] = full[2*j+28+floor((j-1)/14)*28];
		return(mean(pixelvec))
	}
	
	compress = unlist(lapply(1:196,compressFour))
	return(compress)
}

# Plot a multitude of digits
plotTable <- function(numRow,numCol,vec.labels,mat.images){
	vec.uniq = unique(vec.labels)
	par(mfrow=c(numRow,numCol),pty="s",mar = c(0.1,0.1,0.1,0.1))
	for(i in 1:length(vec.uniq)){
		tmpidx = which(vec.labels==vec.uniq[i])
		for(j in 1:length(which(vec.labels==vec.uniq[i]))){
			show_digitsmall(mat.images[tmpidx[j],],asp=TRUE)
		}
	}
}
\end{lstlisting}
}

\subsection{Simulation engine}
\label{sec:appendicies:al:simulations:simengine}

Although the function name is \texttt{AL\_engine}, this code corresponds to the 
simulation engine (all the \texttt{main} simulation file names are preceded by 
a \texttt{AL\_}). The actual active learning engine may be found in 
Appendix~\ref{sec:appendicies:al:simulations:alengine}.

{\setstretch{1,0}
\begin{lstlisting}

AL_engine <- function(X, y, y_unlabeled, al_method,
classifier_method, return_method, iter, n, ...){
	
	stopifnot(nrow(X) == length(y), is.matrix(X), is.factor(y), 
		length(levels(y)) == 2)
	idx <- which(is.na(y_unlabeled))
	stopifnot(length(idx) > 0, all(y[-idx] == y_unlabeled[-idx]), 
	  length(y)==length(y_unlabeled),is.factor(y_unlabeled))
	
	res <- rep(0,iter)
	
	### SET THE COMMITTEE HERE
	cm <- c("rf","nb","pls","svmRadialWeights")
	err<- rep(0,length(cm))
	
	for(i in 1:iter){
		# If QBC, the procedure is a little different....
		if (al_method == "qbc") {
			if (i != 1 & 
			as.character(substitute(classifier_method))=="qbc_majority") {
				# QBC Majority method re-trains committee after the oracle
				# Save computation time by passing those results to QBC algo
				next_sample <- active_learning(X=X,
					y=y_unlabeled,
					almethod=al_method,n=n, 
					committee = cm, 
					isMajority = TRUE, 
					tout = tout, ...)
			} else {
			next_sample <- active_learning(X=X, 
				y=y_unlabeled, almethod=al_method, 
				n=n, committee = cm, ...)
		}
		y_unlabeled[next_sample[[1]]] <- y[next_sample[[1]]]
		
		# Update error and prune committee
		if (i > iter/2) {
			prune <- active_learning(X=X, y=y_unlabeled, 
				almethod="qbc_prune", n = 0, 
				index=next_sample[[1]],
				committee_pred=next_sample[[2]], 
				k = i, err = err, is_prune = TRUE,
				...)
			err <- prune[[1]]
			# check if there's stuff to prune
			if (length(prune[[2]] != 0)) {
				cm <- cm[-unlist(prune[[2]])]
				err <- err[-unlist(prune[[2]])]
			}
		}
		else {
			prune <- active_learning(X=X, y=y_unlabeled, 
				almethod="qbc_prune", n = 0, 
				index=next_sample[[1]],
				committee_pred=next_sample[[2]], 
				k = i, err = err, is_prune = FALSE, 
				...)
			err <- prune[[1]]
		}
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx], committee = cm)
		res[i] <- return_method(tout, X, y, committee = cm)
	}
	# Everything else (not QBC)
	else {
        if (i != 1 & al_method == "us") {
	        # classifier_method re-trains random forest after the oracle
	        # Save computation time by passing those results to US algo
	        # Of course, this only works since classifier = "rf", and the
	        # classifier_method function also uses "rf"
	        next_sample <- active_learning(X,
		        y_unlabeled, al_method, n, 
		        isR = TRUE, tout = tout, ...)
        } else {
	        next_sample <- active_learning(X, y_unlabeled, al_method, n, ...)
        }
 		y_unlabeled[next_sample] <- y[next_sample]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx])
		res[i] <- return_method(tout, X, y)
		}
	}
	res
}
\end{lstlisting}
}

The AL engine without committee pruning (\texttt{AL\_engine\_noprune}) is the 
exact same as \texttt{AL\_engine} with the committee pruning section removed. 
\texttt{AL\_engine\_noprune} is called by the two QBC methods which do not 
utilize committee pruning (see Table~\ref{tab:al:simulations} where 
\textit{C\_Pruning} = F). For the sake of completeness, the code is also 
included below.

{\setstretch{1.0}
\begin{lstlisting}

AL_engine_noprune <- function(X, y, y_unlabeled, al_method,
classifier_method, return_method, iter, n, ...){
	
	stopifnot(nrow(X) == length(y), is.matrix(X), is.factor(y), 
	length(levels(y)) == 2)
	idx <- which(is.na(y_unlabeled))
	stopifnot(length(idx) > 0, all(y[-idx] == y_unlabeled[-idx]), 
	length(y)==length(y_unlabeled),is.factor(y_unlabeled))
	
	res <- rep(0,iter)
	
	### SET THE COMMITTEE HERE
	cm <- c("rf","nb","pls","svmRadialWeights")
	err<- rep(0,length(cm))
	
	for(i in 1:iter){
		# If QBC, the procedure is a little different....
		if (al_method == "qbc") {
			if (i != 1 & 
			as.character(substitute(classifier_method))=="qbc_majority") {
				# QBC Majority method re-trains committee after the oracle
				# Save computation time by passing those results to QBC algo
				next_sample <- active_learning(X=X,
					y=y_unlabeled,
					almethod=al_method,n=n, 
					committee = cm, 
					isMajority = TRUE, 
					tout = tout, ...)
			} else {
			next_sample <- active_learning(X=X,
				y=y_unlabeled, almethod=al_method, 
				n=n, committee = cm, ...)
		}
		y_unlabeled[next_sample[[1]]] <- y[next_sample[[1]]]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx], committee = cm)
		res[i] <- return_method(tout, X, y, committee = cm)
	}
	# Everything else (not QBC, not US)
	else {
		if (i != 1 & al_method == "us") {
			# classifier_method re-trains random forest after the oracle
			# Save computation time by passing those results to US algo
			# Of course, this only works since classifier = "rf", and the
			# classifier_method function also uses "rf"
			next_sample <- active_learning(X, 
				y_unlabeled, al_method, n, 
				isR = TRUE, tout = tout, ...)
		} else {
		next_sample <- active_learning(X, y_unlabeled, al_method, n, ...)
		}
		y_unlabeled[next_sample] <- y[next_sample]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx])
		res[i] <- return_method(tout, X, y)
		}
	}
	res
}
\end{lstlisting}
}

\subsection{AL algorithm engine}
\label{sec:appendicies:al:simulations:alengine}
{\setstretch{1.0}
\begin{lstlisting}

#' Main active learning engine
#'
#' The missing labels in y are denoted by NA.
#' This method takes X as a matrix of all the data
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param almethod the AL method name
#' @param n is the number of unlabeled points to be "pooled"
#' @param ... additional parameters for the active learning method
#'
#' @return an index corresponding to the row of X to learn the label of next
#' @export

active_learning <- function(X, y, almethod = "us", n, ...){

	stopifnot(nrow(X) == length(y), is.matrix(X), any(is.na(y)),
	is.factor(y), length(levels(y)) == 2)
	
	if (n == 0) {
		unlabel_index_c <- which(is.na(y))
	} else unlabel_index_c <- sample(which(is.na(y)), n)
	
	switch(almethod,
		us=uncertainty_sample(X,y,unlabel_index_c, ...),
		rs=random_sample(unlabel_index_c, ns = 1, ...),
		qbc=qbc_sample(X,y,unlabel_index_c, ...),
		qbb=qbb_sample(X,y,unlabel_index_c,...),
		qbc_prune=qbc_prune(X=X, y=y, ...),
		cluster=cluster_sample(X,y,unlabel_index_c, ...)
		)
}
\end{lstlisting}
}

\subsection{Simulator (main)}
\label{sec:appendicies:al:simulations:results}

Finally, the main simulator is what calls the simulation engine 25 times 
(trials) for each active learning method with the parameters described in 
Table~\ref{tab:al:simulations} and collects the results.

{\setstretch{1.0}
\begin{lstlisting}

setwd("---simulation file path---")
# NOTE: AL_header.R loads the simulation R package
# External installation via devtools::install_github("amytian789/thesis-al")
# Local installation via devtools::load_all("---package path---")
source("main/AL_header.R")
source("main/AL_engine.R")
source("main/AL_engine_noprune.R")
source("main/AL_data.R")




################################## Overall classifier and return methods

# MAIN class.model that will train on the data once AL selection is completed
# X and y contain labeled points
classifier_method <- function(X, y, ...) {
	caret::train(X,y,method="rf")
}

# MAIN prediction method for the data once AL selection is completed
# X contains all points to predict
classifier_predict <- function(classifier, X, ...) {
	stats::predict(classifier, X)
}

# Majority Committee Vote classification model
# X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
	tout <- vector("list",length(committee))
	for (i in 1:length(committee)){
		tout[[i]] <- caret::train(X,y,committee[i])
	}
	tout
}

# Generic error ratio
# X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
	p <- stats::predict(classifier, X)
	length(which(p != y))/length(y)
}

# QBC error ratio
# X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
	p <- vector("list",length(committee))
	for (i in 1:length(committee)) {
		p[[i]] <- predict(tout[[i]],newdata=X)
	}
	# Aggregate prediction
	ap <- rep(0,length(y))
	for (i in 1:length(y)){
		temp <- as.numeric(as.character(p[[1]][i]))
		for (j in 1:length(committee)){
			temp <- c(temp,as.numeric(as.character(p[[j]][i])))
		}
		# error checking if a value doesn't appear at all
		if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
			ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
		} else {
			# pick one at random if there is a tie
			if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) ==
			as.numeric(sort(table(temp),decreasing=TRUE)[2])){
				temp <- c(0,1)
				ap[i] <- sample(temp,1)
			} else {
				# Otherwise, insert the first one
				ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
			}
		}
	}
	length(which(ap != y))/length(y)
}




################################ Set up the data

load_mnist()
names(train)

# Randomly select the dataset. a and b are the labels which we want to compare 
# (a,b in [0,10]. We are only interested bivariate classification)
a <- 7
b <- 9
n <- 250 # desired dataset size
init <- 10 # desired number of points to initialize with
set.seed(10)
idx <- c(sample(which(train$y == a),n/2),sample(which(train$y == b),n/2))
X <- train$x[idx,]
X <- t(apply(X,1,compressImg)) # compress from 28x28 to 14x14 pixels
y <- as.factor(train$y[idx]) # y contains the "true" labels. y is never seen by 
the AL algorithms 

# Randomly select the initial points given to the AL algorithms
y_unlabeled <- y
set.seed(10)
y_unlabeled[sample(1:n,n-init)] <- NA

# Visual representation of the data
plotTable(13,20,y,X)

rm(train)




###################################

s <- 15 # Number of random unlabeled points to "pool"
	# n = 0 indicates that the "pool" should sample from all data points
k <- 25 # Number of simulations to run
iter <- 50  # Number of AL algorithm iterations (the "budget")




# Classifier performance given all data is labeled
# pred <- classifier_method(X,y)
# perf_results <- rep(return_method(pred,X,y),iter)
# This has been shown to yield perfect results, so it is commented out

# Uncertainty Sampling
us_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	us_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "us", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter, n = s, classifier = "rf")
	print(c("Trial ",i,"complete"))
}

# Query by Committee with "Majority Committee Vote" model
qbc_majority_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine
	qbc_majority_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = qbc_majority,
		return_method = qbc_m_return, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Committee with "Majority Committee Vote" model
# no pruning
qbc_majority_noprune_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine_noprune
	qbc_majority_noprune_results[i,] <- AL_engine_noprune(X=X,
		y=y, y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = qbc_majority, 
		return_method = qbc_m_return, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Committee with overall "Random Forest" model
qbc_rf_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine
	qbc_rf_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Committee with overall "Random Forest" model
# no pruning
qbc_rf_noprune_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine_noprune
	qbc_rf_noprune_results[i,] <- AL_engine_noprune(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Bagging
qbb_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	qbb_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbb", 
		classifier_method = classifier_method,
		return_method = return_method, 
		iter = iter,n = s,classifier="rf", 
		dis = "vote_entropy", num_class=5, r=0.75)
	print(c("Trial ",i,"complete"))
}

# Min-Max Clustering
cluster_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	cluster_results[i,] <- AL_engine(X=X, y=y,
		y_unlabeled=y_unlabeled, al_method = "cluster", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter,n = s, dis = "euclidean")
	print(c("Trial ",i,"complete"))
}

# Random Sampling
random_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	random_results[i,] <- AL_engine(X, y, y_unlabeled, 
		al_method = "rs", classifier_method, return_method, 
		iter, s)
	print(c("Trial ",i,"complete"))
}

# Average
us_vec <- apply(us_results,2,mean)
random_vec <- apply(random_results,2,mean)
qbc_majority_vec <- apply(qbc_majority_results,2,mean)
qbc_majority_noprune_vec <- apply(qbc_majority_noprune_results,2,mean)
qbc_rf_vec <- apply(qbc_rf_results,2,mean)
qbc_rf_noprune_vec <- apply(qbc_rf_noprune_results,2,mean)
qbb_vec <- apply(qbb_results,2,mean)
cluster_vec <- apply(cluster_results,2,mean)

# Select best QBC output (with pruning)
if (length(which(qbc_majority_vec < qbc_rf_vec)) > 
length(which(qbc_majority_vec > qbc_rf_vec))){
	qbc_prune_vec <- qbc_majority_vec
} else if (length(which(qbc_majority_vec < qbc_rf_vec)) < 
length(which(qbc_majority_vec > qbc_rf_vec))){
	qbc_prune_vec <- qbc_rf_vec
} else{
	# select one at random
	set.seed(1)
	rr <- sample(c(0,1),1)
	if (rr == 0) qbc_prune_vec <- qbc_majority_vec
	else qbc_prune_vec <- qbc_rf_vec
}
# Select best QBC output (with no pruning)
if (length(which(qbc_majority_noprune_vec < qbc_rf_noprune_vec)) > 
length(which(qbc_majority_noprune_vec > qbc_rf_noprune_vec))){
	qbc_noprune_vec <- qbc_majority_noprune_vec
} else if (length(which(qbc_majority_noprune_vec < qbc_rf_noprune_vec)) < 
length(which(qbc_majority_noprune_vec > qbc_rf_noprune_vec))){
	qbc_noprune_vec <- qbc_rf_noprune_vec
} else{
	# select one at random
	set.seed(2)
	rr <- sample(c(0,1),1)
	if (rr == 0) qbc_noprune_vec <- qbc_majority_noprune_vec
	else qbc_noprune_vec <- qbc_rf_noprune_vec
}
# Select best overall QBC output
if (length(which(qbc_prune_vec < qbc_noprune_vec)) > 
length(which(qbc_prune_vec > qbc_noprune_vec))){
	qbc_vec <- qbc_prune_vec
} else if (length(which(qbc_prune_vec < qbc_noprune_vec)) < 
length(which(qbc_prune_vec > qbc_noprune_vec))){
	qbc_vec <- qbc_noprune_vec
} else{
	# select one at random
	set.seed(3)
	rr <- sample(c(0,1),1)
	if (rr == 0) qbc_vec <- qbc_prune_vec
	else qbc_vec <- qbc_noprune_vec
}


################################### Plot the results

date <- Sys.Date()
pdf(file=paste0("results/results_", date, ".PDF"), 
height = 6, width = 10)

### Plot all AL performance
ymax <- max(c(us_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, qbc_vec, ylim=c(0,ymax), lwd=2, type="l", 
	main="AL Error Ratios with Random Forest classification model*", 
	xlab="Iterations", ylab="Error", col = "green")
mtext("*Query by Committee uses Majority Committee Vote classification model")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,
	title="Active Learning method",
	legend = c("Random Sampling","Uncertainty Sampling",
	"Query by Committee (best)","Query by Bagging","Min-Max Clustering"),
	col=c("red","black","green","blue","orange"))

### Plot QBC performance
graphics::plot(1:iter,qbc_majority_vec,ylim=c(0,ymax),lwd=2,type="l"
	,main="Query by Committee AL Error Ratio with various parameters", 
	xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, qbc_majority_noprune_vec, lwd = 2, lty = 2, col = "red")
graphics::lines(1:iter, qbc_rf_vec, lwd = 2, col = "blue")
graphics::lines(1:iter,qbc_rf_noprune_vec, lwd = 2, lty = 2, col = "blue")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,
	title="Main Classifier | Committee Pruning?",
	legend=c("Majority Committee Vote | Yes", "Majority Committee Vote | No",
	"Random Forest | Yes", "Random Forest | No"),
	col=c("red","red","blue","blue"), lty=c(1,2,1,2))

graphics.off()
save.image(file = paste0("results/results_", date, ".RData"))
\end{lstlisting}
}
















\section{Graph summarization engine}
\label{sec:appendicies:gc:engine}

Given graphs $\hat{G^1}=(V^1,E^1)$ and $\hat{G^2}=(V^2,E^2)$, the graph 
summarization engine computes the vector $d^{1,2}$ which 
contains the difference between $\hat{G^1}$ and $\hat{G^2}$ with the various 
summarization metrics described in Section~\ref{sec:gc:methods}. After the 
graph summarization engine code, the distance engine function is also included. 
The distance engine is called by the graph comparison engine in order to 
compute the distance between the two results (one from each graph) for a 
summarization method. Distance methods available are Euclidean distance, L1, 
L2, and Jaccard distance (for use with community only).

{\setstretch{1.0}
\begin{lstlisting}

# g1 and g2 are igraphs
# gSumm is a vector of strings corresponding to graph summarization methods
	### cen_deg = Centrality (degree)
	### cen_clo = Centrality (closeness)
	### cen_bet = Centrality (betweenness)
	### ast = Assortativity
	### com_rw = Community (random walk)
	### com_im = Community (infomap)
	### com_bet = Community (betweenness)
	### dis = distance matrix
	### eco = Edge connectivity
	### edh = Edge density histogram
# distf is the desired difference function

GC_engine <- function(g1, g2, gSumm, distf = "euclidean", ...){

	stopifnot(igraph::is_igraph(g1), igraph::is_igraph(g2), 
		igraph::gorder(g1) == igraph::gorder(g2),
		!is.null(gSumm))
	
	diff <- rep(0,length(gSumm))
	names(diff) <- rep("",length(gSumm))
	i <- 1
	
	# Centrality (degree)
	if ("cen_deg" %in% gSumm){
		a <- igraph::centr_degree(g1)$centralization
		b <- igraph::centr_degree(g2)$centralization
		
		diff[i] <- dist_engine(a,b,distf)
		names(diff)[i] <- "cen_deg"
		i <- i + 1
	}
	
	# Centrality (closeness)
	if ("cen_clo" %in% gSumm){
		a <- igraph::centr_clo(g1)$centralization
		b <- igraph::centr_clo(g2)$centralization
		
		diff[i] <- dist_engine(a,b,distf)
		names(diff)[i] <- "cen_clo"
		i <- i + 1
	}
	
	# Centrality (betweenness)
	if ("cen_bet" %in% gSumm){
		a <- igraph::centr_betw(g1)$centralization
		b <- igraph::centr_betw(g2)$centralization
		
		diff[i] <- dist_engine(a,b,distf)
		names(diff)[i] <- "cen_bet"
		i <- i + 1
	}
	
	# Assortativity
	if ("ast" %in% gSumm){
		a <- igraph::assortativity_degree(g1)
		b <- igraph::assortativity_degree(g2)
		
		if (is.nan(a)) a <- 0
		if (is.nan(b)) b <- 0
		
		diff[i] <- dist_engine(a,b,distf)
		names(diff)[i] <- "ast"
		i <- i + 1
	}
	
	# Community (random walk)
	if ("com_rw" %in% gSumm){
		a <- igraph::membership(igraph::cluster_walktrap(
			g1,steps=igraph::gorder(g1)/2))
		b <- igraph::membership(igraph::cluster_walktrap(
			g2,steps=igraph::gorder(g2)/2))
		
		diff[i] <- dist_engine(a,b,dist="jaccard")
		names(diff)[i] <- "com_rw"
		i <- i + 1
	}
	
	# Community (infomap)
	if ("com_im" %in% gSumm){
		a <- igraph::membership(igraph::cluster_infomap(g1))
		b <- igraph::membership(igraph::cluster_infomap(g2))
		
		diff[i] <- dist_engine(a,b,dist="jaccard")
		names(diff)[i] <- "com_im"
		i <- i + 1
	}
	
	# Community (betweenness)
	if ("com_bet" %in% gSumm){
		a <- igraph::membership(igraph::cluster_edge_betweenness(g1))
		b <- igraph::membership(igraph::cluster_edge_betweenness(g2))
		
		diff[i] <- dist_engine(a,b,dist="jaccard")
		names(diff)[i] <- "com_bet"
		i <- i + 1
	}
	
	# Distance matrix 
	if ("dis" %in% gSumm){
	    a <- distances(g1)
	    b <- distances(g2)
	    
	    # change from matrix -> vector for distance computation (by column)
	    # keep only 1 side of the matrix (both sides are the same)
	    # don't keep the values in the middle (since it's the same node)
	    a[a == Inf] <- 0
	    a <- a[upper.tri(a)]
	    b[b == Inf] <- 0
	    b <- b[upper.tri(b)]
	    
	    diff[i] <- dist_engine(a,b,distf) / (dist_engine(
		    rep(0,igraph::gorder(g1)-1),
		    seq(1,igraph::gorder(g1)-1,by=1),distf)
	    names(diff)[i] <- "dis"
	    i <- i + 1
	}
	
	# Edge connectivity 
	if ("eco" %in% gSumm){
		if (min(igraph::degree(g1)) != 0){
		a <- igraph::edge_connectivity(g1) / min(igraph::degree(g1))
		} else a <- 0
		if (min(igraph::degree(g2)) != 0){
		b <- igraph::edge_connectivity(g2) / min(igraph::degree(g2))
		} else b <- 0
		
		diff[i] <- dist_engine(a,b,distf)
		names(diff)[i] <- "eco"
		i <- i + 1
	}
	
	# Edge density histogram
	if ("edh" %in% gSumm){
		# histograms should be on the same scale. 
		# Use Freedman-Diaconis rule to determine bin width
		dd <- max(igraph::degree(g1),igraph::degree(g2))
		bw <- 2 * stats::IQR(igraph::degree(g1)) / igraph::gorder(g1)^(1/3)
		a <- graphics::hist(igraph::degree(g1),plot=FALSE,
			breaks=seq(0,dd+bw,by=bw))$counts / igraph::gorder(g1)
		b <- graphics::hist(igraph::degree(g2),plot=FALSE,
			breaks=seq(0,dd+bw,by=bw))$counts / igraph::gorder(g2)
		
		diff[i] <- dist_engine(a,b,distf)
		names(diff)[i] <- "edh"
		i <- i + 1
	}
	
	diff
}

######## Engine to call various distance computation methods
dist_engine <- function(a,b,dist = "euclidean", ...){
	switch(dist,
	euclidean=dist_euc(a,b),
	l1=dist_l1(a,b),
	l2=dist_l2(a,b),
	jaccard=dist_jac(a,b)
	)
}

# Euclidean distance
dist_euc <- function(a,b){
	sqrt( sum( mapply( function(x,y) (x-y)^2, a, b)))
}

# L1: Sum of absolute differences
dist_l1 <- function(a,b){
	sum ( mapply ( function(x,y) abs(x-y), a, b))
}

# L2: Sum of squared differences
dist_l2 <- function(a,b){
	sum ( mapply ( function(x,y) (x-y)^2, a, b))
}

# Jaccard distance
dist_jac <- function(a,b){
	1-clusteval::cluster_similarity(a,b,similarity="jaccard")
}
\end{lstlisting}

}


\section{Similarity selection}
\label{sec:appendicies:gc:similarity}

Given a base graph $\hat{G}$ and a set of graphs, the similarity selection 
method selects the graph $\hat{G}^*$ that is most similar to $\hat{G}$. The 
method is 
described in Section~\ref{sec:gc:simulations}.

{\setstretch{1.0}
\begin{lstlisting}

# Search for the "most similar" graph pair
# (characterized by having the lowest differences across the board)
#
# gc is a list of the differences among various graph pairs
# base is the index with which to start the search

GC_selection <- function(gc, base = 1){
	
	stopifnot(!is.null(gc))
	
	idx <- base
	old_idx <- 0
	while (old_idx != idx){
		old_idx <- idx
		idx <- best_c(gc,idx)
		#print(paste(old_idx,idx))
	}
	idx
}

# Select the next idx candidate given current best candidate
best_c <- function(gc, cand){

	for (i in 1:length(gc)){
		if (i != cand){
			if (sum(gc[[cand]] < gc[[i]]) < sum(gc[[cand]] > gc[[i]])){
				return( i )
			} else if (sum(gc[[cand]] < gc[[i]]) > sum(gc[[cand]] > gc[[i]])){
				# do nothing since the current candidate is better
			} else{
				return( sample(c(cand,i),1) )
			}
		}
	}
	return( cand )  
}
\end{lstlisting}
}


\section{GC simulation study}
\label{sec:appendicies:gc:simulations}

The graph comparison simulation study, which is used to demonstrate the 
viability of the 
proposed similarity selection model, is described in 
Section~\ref{sec:gc:simulations:algorithm}.

{\setstretch{1.0}
\begin{lstlisting}

setwd("---simulation file path---")
source("GC_engine.R")
source("dist_engine.R")
source("GC_selection.R")

library(igraph)
library(clusteval)

################################## Random sample given PDF

randomdraw <- function(n, prob){
	if(round(sum(prob),1) != 1 | length(which(prob<0)) > 0) 
	stop("Probability must be between 0 and 1")
	
	runningsum <- 0
	s <- runif(n)
		for (i in 1:n){
			for (j in 1:length(prob)){
				runningsum <- runningsum + prob[j]
				if (s[i] < runningsum){
				s[i] <- j
				break
			} 
		}
	}
	s
}

################################## Run simulations

# Create base graph
set.seed(10)
ss <- 50 # desired sample size (# of edges)
bg <- igraph::sample_gnm(20, ss)
bg_e <- igraph::as_edgelist(bg)

# Setting parameters
gSumm <- c("cen_deg","cen_clo","cen_bet","ast",
	"com_rw","com_im","com_bet","dis","eco","edh")
distf <- "l2"

msg <- rep(0,1000)
for (i in 1:1000) {
	set.seed(i)
	
	# g1: randomly swap 20% edges. 
	# Weight of each node is proportional to its degree
	g1 <- bg
	for (j in 1:(ss/5)) {
		idx <- sample(igraph::as_edgelist(g1),1)
		g1 <- igraph::delete_edges(g1,idx)
		prob <- igraph::degree(g1) / sum(igraph::degree(g1))
		nva <- sample(seq(1,length(prob),1),1)
		nvb <- randomdraw(1, prob)
		# make sure edges are not repeated
		while (g1[nva,nvb] == 1 | nva == nvb) nvb <- randomdraw(1,prob)
		g1 <- igraph::add_edges(g1, c(nva,nvb))
	}
	
	# g2: randomly swap 100% edges. 
	# Weight of each node is proportional to its degree
	g2 <- bg
	for (j in 1:(ss)) {
		idx <- sample(igraph::as_edgelist(g2),1)
		g2 <- igraph::delete_edges(g2,idx)
		prob <- igraph::degree(g2) / sum(igraph::degree(g2))
		nva <- sample(seq(1,length(prob),1),1)
		nvb <- randomdraw(1, prob)
		# make sure edges are not repeated
		while (g2[nva,nvb] == 1 | nva == nvb) nvb <- randomdraw(1,prob)
		g2 <- igraph::add_edges(g2, c(nva,nvb))
	}
	
	# Compute difference between (bg,g1), (bg,g2)
	gc <- vector("list",2)
	gc[[1]] <- GC_engine(g1=bg,g2=g1,gSumm=gSumm,distf=distf)
	gc[[2]] <- GC_engine(g1=bg,g2=g2,gSumm=gSumm,distf=distf)
	
	# Select the most similar graph pair
	msg[i] <- GC_selection(gc = gc, base = 1)
}
p1 <- length(which(msg == 1)) / length(msg)
p2 <- length(which(msg == 2)) / length(msg)
cat("Prob. of selecting (bg,g1):",p1,"\nProb. of selecting (bg,g2):", p2)

################################## Plot simulation graphs

bg$layout <- layout_in_circle # base graph
g1$layout <- layout_in_circle # from trial 1000 (seed = 1000)
g2$layout <- layout_in_circle # from trial 1000 (seed = 1000)

par(mfrow=c(1,3))
plot(bg)
plot(g1)
plot(g2)
\end{lstlisting}
}




















\section{Stock selection}
\label{sec:appendicies:usage:stockselection}

Given a correlation graph $G=(V,E)$, the program selects $k < |V|$ stocks 
(nodes) 
such that they are as uncorrelated with each other as possible. 
See Section~\ref{sec:usage:stockselection} and 
Algorithm~\ref{alg:usage:stockselection2}.

{\setstretch{1.0}
\begin{lstlisting}

# g_num is the numerical graph as an adjacency matrix
# k is the number of stocks to select
# dd is a vector of sample average price differences over time
# pd is the "drift" threshold ratio
# vv is a vector of sample average standard deviation
# pv is the "volatility" threshold ratio
#
# Returns a vector of selected stock tickers

stock_selection <- function(g_num, k, dd, pd = 0, vv, pv = 0){

	stopifnot(ncol(g_num) > k, pd >= 0, pv >= 0)
	
	# A) Drift
	rmv <- c()
	avgret <- sum(dd) / ncol(dd)
	for (i in 1:ncol(g_num)){
		if(dd[i] <= avgret * pd){
			rmv <- c(rmv, i)
		}
	}
	
	# B) Volatility
	avgvol <- sum(vv) / ncol(vv)
	for (i in 1:ncol(g_num)){
		if(vv[i] <= avgvol * pv){
			if(!(i %in% rmv)){
				rmv <- c(rmv, i)
			}
		}
	}
	g_num <- g_num[-rmv,-rmv]
	
	#print(g_num)
	
	# C) Main selection
	z <- combn(ncol(g_num),k)
	min <- Inf
	idx <- 0 
	for (i in 1:ncol(z)){
		c <- 0 # number of connections
		for (j in 1:(k-1)){
			for (l in (j+1):k){
				c <- c + g_num[z[j,i],z[l,i]]
			}
			if (c < min){
				min <- c
				idx <- i
			} else if (c == min){
				if (runif(1,0,1) > 0.5){
					min <- c
					idx <- i
				}
			} else{
				# do nothing
			}
		}
	}
	idx_s <- c()
	for (i in 1:length(z[,idx])){
		idx_s <- c(idx_s,colnames(g_num)[z[i,idx]])
	}
	idx_s
}
\end{lstlisting}
}










\section{Final application}
\label{sec:appendicies:usage:newanalysis}

Section~\ref{sec:usage:newanalysis} details the application procedure that is 
implemented below.

{\setstretch{1.0}
\begin{lstlisting}

setwd("---application file path---")
source("stock_selection.R")

library(Rsafd)
library(timeDate)
library(energy)
library(igraph)
library(clusteval)




################################## Read in data.

rd <- read.csv(file="HealthcareETF_prices.csv",sep=",",header=TRUE)
date <- as.timeDate(rd[,1])
data <- rd[,-1]

# avg drift and vol (computed in Excel)
dd <- read.csv(file="HealthcareETF_drift.csv",sep=",",header=TRUE)
vv <- read.csv(file="HealthcareETF_vol.csv",sep=",",header=TRUE)

# Remove seasonal and trend components of data
for (i in 1:ncol(data)){
	temp.ts <- Rsafd::timeSeries(data=data[,i],positions=date)
	# 261 is the avg number of workdays in a year
	temp.stl <- Rsafd::sstl(temp.ts, FREQ=261) 
	data[,i] <- temp.stl$rem@data
}

# Split data into testing and training sets
# Assume we are standing in the middle of the 1998 and 2014
data_train <- as.matrix(data[1:(nrow(data)/2),])
data_test <- as.matrix(data[(nrow(data)/2+1):nrow(data),])




################################## Create numerical graphs

# correlation threshold
t <- 0.15

# Pearson's correlation graph
m_pear <- matrix(0, ncol(data_train), ncol(data_train))
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		m_pear[i,j] <- stats::cor.test(data_train[,i],
			data_train[,j],method="pearson")$estimate
	}
}
g_pear <- igraph::make_empty_graph(n = ncol(data_train), directed = FALSE)
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		if (abs(m_pear[i,j]) > t){
			g_pear <- igraph::add_edges(g_pear, c(i,j))
		}
	}
}

# Spearman's correlation graph
m_spea <- matrix(0, ncol(data_train), ncol(data_train))
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		m_spea[i,j] <- stats::cor.test(data_train[,i],
			data_train[,j],method="spearman")$estimate
	}
}
g_spea <- igraph::make_empty_graph(n = ncol(data_train), directed = FALSE)
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		if (abs(m_spea[i,j]) > t){
			g_spea <- igraph::add_edges(g_spea, c(i,j))
		}
	}
}

# Kendall's correlation graph
m_ktau <- matrix(0, ncol(data_train), ncol(data_train))
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		m_ktau[i,j] <- stats::cor.test(data_train[,i],
			data_train[,j],method="kendall")$estimate
	}
}
g_ktau <- igraph::make_empty_graph(n = ncol(data_train), directed = FALSE)
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		if (abs(m_ktau[i,j]) > t){
			g_ktau <- igraph::add_edges(g_ktau, c(i,j))
		}
	}
}

# Distance correlation graph
m_dist <- matrix(0, ncol(data_train), ncol(data_train))
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		m_dist[i,j] <- energy::dcor.ttest(data_train[,i],
			data_train[,j])$estimate
	}
}
g_dist <- igraph::make_empty_graph(n = ncol(data_train), directed = FALSE)
for (i in 1:(ncol(data_train)-1)){
	for (j in (i+1):ncol(data_train)){
		if (abs(m_dist[i,j]) > t){
			g_dist <- igraph::add_edges(g_dist, c(i,j))
		}
	}
}

date <- Sys.Date()
save.image(file = paste0("corrgraphs_", date, ".RData"))




################################## Visualization system

devtools::install_github("linnylin92/graphicalModelEDA", 
	ref = "kevin",subdir = "graphicalModelEDA", force = T)
library(graphicalModelEDA)

# Run the VS
control_obj <- graphicalModelEDA::visualizer_control()
control_obj$parameter_list$initial_set <- 10 # faster initialization
res <- graphicalModelEDA::visualizer_system(data_train, trials = 15, 
	visualizer_control = control_obj, asp = T)

# Visual graph
g_visg <- graphicalModelEDA:::visual_graph(res, data_train)
plot(g_visg)

# Heat map
plot(res$results)

# Association navigator
plot(res$results, aes.list = aes.buja())

# Evaluate 100 random pairs and their associated plots
search_res <- graphicalModelEDA::automated_search(res,data_train)
# Histogram of predicted probabilities of correlation
plot(search_res) 
# Ordered predicted probabilities w/ the scatterplot pairs
cbind(search_res$pairs[order(search_res$values),], sort(search_res$values)) 
par(mfrow=c(2,3))
# Plot the top 5 most "interesting" pairs
graphicalModelEDA::selective_plot(search_res, data_train, 
	selection = function(vec){ifelse(vec >= sort(vec, 
	decreasing = TRUE)[5], TRUE, FALSE)})

date <- Sys.Date()
save.image(file = paste0("visgraph_", date, ".RData"))




################################## Graph comparison

setwd("---GC file path---")
source("GC_engine.R")
source("dist_engine.R")
source("GC_selection.R")
setwd("---application file path---")

# Setting parameters
gSumm <- c("cen_deg","cen_clo","cen_bet","ast",
	"com_rw","com_im","com_bet","dis","eco","edh")
distf <- "l2"

# Compute difference between all graph pairs (g_visg is the "base")
gc <- vector("list",4)
gc[[1]] <- GC_engine(g1=g_visg,g2=g_pear,gSumm=gSumm,distf=distf)
gc[[2]] <- GC_engine(g1=g_visg,g2=g_spea,gSumm=gSumm,distf=distf)
gc[[3]] <- GC_engine(g1=g_visg,g2=g_ktau,gSumm=gSumm,distf=distf)
gc[[4]] <- GC_engine(g1=g_visg,g2=g_dist,gSumm=gSumm,distf=distf)

# Select the most similar graph pair
idx <- GC_selection(gc = gc, base = 3)

if (idx == 1){
	print("Pearson's")
} else if (idx == 2){
	print("Spearman's")
} else if (idx == 3){
	print("Kendall's")
} else{
	print("Distance")
}




################################## Stock selection

# Set parameters
k <- 5 # number of stocks to select
pd <- 2/3 # drift threshold ratio 
pv <- 2/3 # vol threshold ratio

# Pearson's
set.seed(10)
a_pear <- igraph::as_adjacency_matrix(g_pear)
colnames(a_pear) <- colnames(data)
s_pear <- stock_selection(a_pear, k, dd, pd, vv, pv)
s_pear

# Spearman's
set.seed(10)
a_spea <- igraph::as_adjacency_matrix(g_spea)
colnames(a_spea) <- colnames(data)
s_spea <- stock_selection(a_spea, k, dd, pd, vv, pv)
s_spea

# Kendall's
set.seed(10)
a_ktau <- igraph::as_adjacency_matrix(g_ktau)
colnames(a_ktau) <- colnames(data)
s_ktau <- stock_selection(a_ktau, k, dd, pd, vv, pv)
s_ktau

# Distance
set.seed(10)
a_dist <- igraph::as_adjacency_matrix(g_dist)
colnames(a_dist) <- colnames(data)
s_dist <- stock_selection(a_dist, k, dd, pd, vv, pv)
s_dist

# Visual
set.seed(10)
a_visg <- igraph::as_adjacency_matrix(g_visg)
colnames(a_visg) <- colnames(data)
s_visg <- stock_selection(a_visg, k, dd, pd, vv, pv)
s_visg

date <- Sys.Date()
save.image(file = paste0("portfolios_", date, ".RData"))




################################## Compare future performance 

# ON EXCEL: Use testing set to do this 
\end{lstlisting}
}