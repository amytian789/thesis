\chapter{Implementation\label{ch:implementation}}

\lstset{basicstyle=\ttfamily\footnotesize,xleftmargin=0cm,breaklines=true,language=R}

\section{Code for figure~\ref{fig:intro:meplot}, left}
\label{sec:appendicies:me1plot}
{\setstretch{1.0}
\begin{lstlisting}
# Generate a reproducible dataset and scale to [0,1]
set.seed(10)
x <- seq(0, 1, length.out = 100)
y <- rnorm(100)
y <- (y-min(y))/(max(y)-min(y))

# Sort the noise
y <- sort(y)
y <- y[c(seq(1,99,length.out=50), seq(100,2,length.out=50))]

# Local swapping
for(i in 4:96){
	y[(i-3):(i+3)] <- y[sample((i-3):(i+3))]
}

idx <- sample(1:100)
x <- x[idx]; y <- y[idx]

##################### Numerical feedback

## Fit linear regression
fitlm <- lm(y ~ x)
anova(fitlm)

## See if any coefficients are significant
summary(fitlm)

## See if residuals are normally-distributed
shapiro.test(fitlm$residuals)

## Correlation is not significantly different from zero
cor.test(x, y)

#################### Visual feedback
plot(x, y, pch = 16, cex = 2)
\end{lstlisting}
}


\section{Code for figure~\ref{fig:intro:meplot}, right}
\label{sec:appendicies:me2plot}
{\setstretch{1.0}
\begin{lstlisting}
## Generate a reproducible dataset
set.seed(10)
n <- 50
x <- sort(rnorm(n))
sd.vec <- c(seq(1, 1.5, length.out = 50), seq(1.5, 1, length.out = 50))
y <- -x + 0.5*rnorm(n, sd = sd.vec)
y <- scale(y)

y[c(1,5,10)] <- min(y)
y[c(n-10, n-5, n)] <- max(y)

##################### Numerical feedback

## Fit linear regression
fitlm <- lm(y ~ x)
anova(fitlm)

## See if any coefficients are significant
summary(fitlm)

## See if residuals are normally-distributed
shapiro.test(fitlm$residuals) 

## Correlation is not significantly different from zero
cor.test(x, y)

#################### Visual feedback
plot(x,y, pch = 16, cex = 2)
\end{lstlisting}
}


\section{Code for figure~\ref{fig:visualizer:cdf}}
\label{sec:appendicies:cdf}
{\setstretch{1.0}
\begin{lstlisting}
# Generate the dataset
set.seed(10)
n <- 500
x <- rnorm(n)
y <- rnorm(n)

# Plot data and apply CDF
par(mfrow=c(1,2))
plot(x,y, pch = 16, cex = 2)
plot(pnorm(x),pnorm(y),pch = 16, cex = 2)
\end{lstlisting}
}




\section{Uncertainty sampling implementation}
\label{sec:appendicies:al:uncertainty}

Refer to Algorithm \ref{alg:al:methods:uncertainty}. 
{\setstretch{1.0}
\begin{lstlisting}

#' Uncertainty Sampling with bivariate labels
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param classifier the classifier name
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

uncertainty_sample <- function(X, y, unlabel_index_c, classifier,
	isR = FALSE, tout = NULL, ...){

	if (length(classifier) > 1 || missing(classifier) || is.null(classifier) || 
	is.na(classifier)) {
		stop("A single classifier is required for uncertainty sampling")
	}
	if (isR & is.null(tout)) {
		stop("Re-feed classifier_method return to next uncertainty_sample call")
	}	
	
	# Check that the classifier is compatible with uncertainty sampling
	c <- try(caret::modelLookup(classifier))
	if (!any(c$probModel)) {
		stop(classifier," must return posterior probabilities")
	}
	
	# Split X and y to retrieve labeled and unlabeled pairs
	unlabel_index <- which(is.na(y))
	x_lab <- X[-unlabel_index,]
	y_lab <- y[-unlabel_index]
	x_ulab <- X[unlabel_index_c,]
	
	if (!isR) {
		tout <- caret::train(x_lab,y_lab,classifier)
		p <- as.matrix(stats::predict(tout, newdata=x_ulab, type="prob"))
	} else {
		# Reuse the trained classifier from the classifier_method call
		# Of course, this only works since classifier = "rf", and the
		# classifier_method function also uses "rf"
		p <- as.matrix(stats::predict(tout, newdata=x_ulab, type="prob"))
	}
	
	# Return corresponding X index of posterior closest to 0.5
	p <- apply(p, 1, function(x) abs(x[1]-0.5))
	index <- unlabel_index_c[which(p == min(p))]
	if (length(index) > 1) index <- sample(index,1)
	index
}
\end{lstlisting}
}

\section{Query by committee implementation}
\label{sec:appendicies:al:qbc}

Refer to Algorithm \ref{alg:al:methods:qbc2}. 
This implementation contains the functions for query selection and pruning. 
These functions are called by the main simulation engine (Appendix 
~\ref{sec:appendicies:al:simulations:simengine}) for the QBC method. 
The simulation engine is coded in such as way that it acts as the skeleton 
of the entire algorithm in Section~\ref{sec:al:methods:qbc}.
\subsection{Query selection}
{\setstretch{1.0}
\begin{lstlisting}

#' Query by Committee
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param committee the list of committee classifiers
#' @param dis is the disagreement measure between committee classifications
#' @param isMajority is if overall classifier Majority Vote or Random Forest
#' @param tout is a list of trained classifiers from Majority Vote computation 
#' @param ... additional parameters for the active learning method
#'
#' @return a list with: an index to query AND committee predictions
#' @export

qbc_sample <- function(X, y, unlabel_index_c, committee,
	dis = "vote_entropy", isMajority = FALSE, tout= NULL, ...){
	
	if (missing(committee)||is.null(committee)) stop("A committee is required")
	if (isMajority & is.null(tout)) {
		stop("Re-feed the majority vote return to the next QBC_sample call")
	}
	
	unlabel_index <- which(is.na(y))
	x_lab <- X[-unlabel_index,]
	y_lab <- y[-unlabel_index]
	x_ulab <- X[unlabel_index_c,]
	p <- vector("list",length(committee))
	
	if (!isMajority) {
		for (i in 1:length(committee)) {
			tout <- caret::train(x_lab,y_lab,committee[i])
			p[[i]] <- predict(tout, newdata=x_ulab)
		}
	} else {
		# Reuse the trained classifiers from the majority vote call
		for (i in 1:length(committee)) {
			p[[i]] <- predict(tout[[i]], newdata=x_ulab)
		}
	}

	# Compute disagreement (functions from the activelearning package)
	d <- switch(dis,
		vote_entropy=vote_entropy(p),
		post_entropy=post_entropy(p),
		kullback=kullback(p)
		)
	
	index <- unlabel_index_c[which(d == max(d))]
	if (length(index) > 1) index <- sample(index,1)
	# Gather each committee's prediction
	pre <- rep(0,length(committee))
	for (i in 1:length(committee)) {
		# Predict function returns a factor
		pre[i] <- 
		as.numeric(as.character(p[[i]][which(unlabel_index_c==index)]))
	}
	
	list(index, pre)
}
\end{lstlisting}
}



\subsection{Committee pruning}
{\setstretch{1.0}
\begin{lstlisting}

#' Query by Committee (committee pruning function)
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param index is the classification of X[index,] which was queried
#' @param committee_pred is the list of committee predictions for index
#' @param k is the current iteration number that the AL_engine is on
#' @param pt is the pruning threshold (any error value above it is pruned)
#' @param err in (0 best,1 worst) is the committee's error-to-iteration ratio
#' @param is_prune is TRUE when pruning is desired, FALSE when not
#' @param ... additional parameters for the active learning method
#'
#' @return a list with: updated error AND indices to delete from the committee
#' @export

qbc_prune <- function(X,y,index,committee_pred,k,pt = 0.5,err,is_prune,...){

	if (missing(err) || is.null(err) || is.na(err)) {
		stop("Committee error ratio is required for QBC pruning")
	}
	prune <- vector() # Do not know how long prune will be until the end
	# Do not prune if committee size is 1 or if it's the first round
	if (length(committee_pred) == 1 | k == 1) {
		list(err, prune)
	} else {
		# Update error value
		for (i in 1:length(committee_pred)) {
			if (committee_pred[i] == y[index]) iv <- 0 else iv <- 1
			err[i] <- err[i] + (iv - err[i])/k
			if (err[i] > pt & is_prune) {
				prune <- c(prune,i)
			}
		}
		list(err, prune)
	}
}
\end{lstlisting}
}

\section{Vote entropy implementation}
\label{sec:appendicies:al:entropy}

Refer to Section \ref{sec:al:methods:qbc}.
{\setstretch{1.0}
\begin{lstlisting}

#' Disagreement method (from activelearning package)
#' @importFrom itertools2 izip
#' @importFrom entropy entropy

vote_entropy <- function(x, type='class', entropy_method='ML'){

	it <- do.call(itertools2::izip, x)
	disagreement <- sapply(it, function(obs) {
		entropy::entropy(table(unlist(obs)), method=entropy_method)
	})
	disagreement
}
\end{lstlisting}
}



\section{Query by bagging implementation}
\label{sec:appendicies:al:bagging}

Refer to Algorithm \ref{alg:al:methods:bagging}. 
{\setstretch{1.0}
\begin{lstlisting}

#' Query by Bagging
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param classifier the name of a classification model
#' @param dis is the disagreement measure between committee classifications
#' @param num_class is the number of desired committee members
#' @param r in (0,1). r*(labeled set) = training set for each num_class round
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

qbb_sample <- function(X, y, unlabel_index_c, classifier, 
	dis = "vote_entropy", num_class, r, ...){

	if(r<=0 || r>=1) stop("r must be in (0,1)")
	
	x_ulab <- X[unlabel_index_c,]
	
	# Randomly sample from the labeled set to create a classifier
	label_index <- which(!is.na(y))
	committee <- vector("list",num_class)
	for (i in 1:num_class) {
		idx <- sample(label_index,round(length(label_index)*r,0))
		committee[[i]] <- caret::train(X[idx,],y[idx],classifier)
	}
	
	# Utilize the resulting classifiers as a committee
	p <- vector("list",length(committee))
	for (i in 1:length(committee)) {
		p[[i]] <- stats::predict(committee[[i]], x_ulab)
	}
	
	# Compute disagreement (functions from the activelearning package)
	d <- switch(dis,
		vote_entropy=vote_entropy(p),
		post_entropy=post_entropy(p),
		kullback=kullback(p)
		)
	
	index <- unlabel_index_c[which(d == max(d))]
	if (length(index) > 1) index <- sample(index,1)
	index
}
\end{lstlisting}
}

\section{Min-max clustering implementation}
\label{sec:appendicies:al:clustering}

Refer to Algorithm \ref{alg:al:methods:clustering}. 
{\setstretch{1.0}
\begin{lstlisting}

#' Min-Max Clustering
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param unlabel_index_c is a vector of n pre-selected (pooled) indices
#' @param dis is the distance measure between data
#' @param ... additional parameters for the active learning method
#'
#' @return an index to query
#' @export

cluster_sample <- function(X,y,unlabel_index_c,dis="euclidean",...){

	label_index <- which(!is.na(y))
	x_lab <- X[label_index,]
	y_lab <- y[label_index]
	x_ulab <- X[unlabel_index_c,]
	y_ulab <- y[unlabel_index_c]
	
	# Select the point furthest from the labeled set
	q <- rep(0,length(y_ulab))
	for (i in 1:length(y_ulab)) {
		min <- Inf
		for (j in 1:length(y_lab)) {
			temp <- cs_distance(X[unlabel_index_c[i],],X[label_index[j],],dis)
			if (min > temp) min <- temp
		}
		q[i] <- min
	}
	index <- unlabel_index_c[which(q==max(q))]
	if (length(index) > 1) index <- sample(index,1)
	index
}

# Main distance engine
cs_distance <- function(a,b,dis = "euclidean"){
	d <- switch(dis,
		euclidean=cs_euclidean_distance(a,b)
		)
}

# Euclidean Distance
cs_euclidean_distance <- function(a,b) {
	sqrt( sum( mapply( function(x,y) (x-y)^2, a, b)))
}
\end{lstlisting}
}

\section{Simulation implementation}
\label{sec:appendicies:al:simulations}

Refer to Section \ref{sec:al:simulations}.
\subsection{MNIST data}
\label{sec:appendicies:al:simulations:data}
{\setstretch{1.0}
\begin{lstlisting}

# Load the MNIST dataset
load_mnist <- function() {
	load_image_file <- function(filename) {
		ret = list()
		f = file(filename,'rb')
		readBin(f,'integer',n=1,size=4,endian='big')
		ret$n = readBin(f,'integer',n=1,size=4,endian='big')
		nrow = readBin(f,'integer',n=1,size=4,endian='big')
		ncol = readBin(f,'integer',n=1,size=4,endian='big')
		x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
		ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
		close(f)
		ret
	}
	load_label_file <- function(filename) {
		f = file(filename,'rb')
		readBin(f,'integer',n=1,size=4,endian='big')
		n = readBin(f,'integer',n=1,size=4,endian='big')
		y = readBin(f,'integer',n=n,size=1,signed=F)
		close(f)
		y
	}
	train <<- load_image_file('mnist/train-images-idx3-ubyte')	
	train$y <<- load_label_file('mnist/train-labels-idx1-ubyte')
}

# Plot a single digit
show_digitsmall <- function(arr196, col=gray(12:1/12), ...) {
	image(matrix(arr196, nrow=14)[,14:1], col=col, ...)
}

# Compress the MNIST dataset from 28x28 to 14x14
compressImg <- function(full){
	compressFour <- function(j){
		pixelvec = rep(NA,4)
		pixelvec[1] = full[2*j-1+floor((j-1)/14)*28];
		pixelvec[2] = full[2*j+floor((j-1)/14)*28];
		pixelvec[3] = full[2*j-1+28+floor((j-1)/14)*28];
		pixelvec[4] = full[2*j+28+floor((j-1)/14)*28];
		return(mean(pixelvec))
	}
	
	compress = unlist(lapply(1:196,compressFour))
	return(compress)
}

# Plot a multitude of digits
plotTable <- function(numRow,numCol,vec.labels,mat.images){
	vec.uniq = unique(vec.labels)
	par(mfrow=c(numRow,numCol),pty="s",mar = c(0.1,0.1,0.1,0.1))
	for(i in 1:length(vec.uniq)){
		tmpidx = which(vec.labels==vec.uniq[i])
		for(j in 1:length(which(vec.labels==vec.uniq[i]))){
			show_digitsmall(mat.images[tmpidx[j],],asp=TRUE)
		}
	}
}
\end{lstlisting}
}

\subsection{Simulation engine}
\label{sec:appendicies:al:simulations:simengine}

Although the function name is \texttt{AL\_engine}, this code corresponds to the 
simulation engine (all the \texttt{main} simulation file names are preceded by 
a \texttt{AL\_}). The actual active learning engine may be found in 
Appendix~\ref{sec:appendicies:al:simulations:alengine}.

{\setstretch{1,0}
\begin{lstlisting}

AL_engine <- function(X, y, y_unlabeled, al_method,
classifier_method, return_method, iter, n, ...){
	
	stopifnot(nrow(X) == length(y), is.matrix(X), is.factor(y), 
		length(levels(y)) == 2)
	idx <- which(is.na(y_unlabeled))
	stopifnot(length(idx) > 0, all(y[-idx] == y_unlabeled[-idx]), 
	  length(y)==length(y_unlabeled),is.factor(y_unlabeled))
	
	res <- rep(0,iter)
	
	### SET THE COMMITTEE HERE
	cm <- c("rf","nb","pls","svmRadialWeights")
	err<- rep(0,length(cm))
	
	for(i in 1:iter){
		# If QBC, the procedure is a little different....
		if (al_method == "qbc") {
			if (i != 1 & 
			as.character(substitute(classifier_method))=="qbc_majority") {
				# QBC Majority method re-trains committee after the oracle
				# Save computation time by passing those results to QBC algo
				next_sample <- active_learning(X=X,
					y=y_unlabeled,
					almethod=al_method,n=n, 
					committee = cm, 
					isMajority = TRUE, 
					tout = tout, ...)
			} else {
			next_sample <- active_learning(X=X, 
				y=y_unlabeled, almethod=al_method, 
				n=n, committee = cm, ...)
		}
		y_unlabeled[next_sample[[1]]] <- y[next_sample[[1]]]
		
		# Update error and prune committee
		if (i > iter/2) {
			prune <- active_learning(X=X, y=y_unlabeled, 
				almethod="qbc_prune", n = 0, 
				index=next_sample[[1]],
				committee_pred=next_sample[[2]], 
				k = i, err = err, is_prune = TRUE,
				...)
			err <- prune[[1]]
			# check if there's stuff to prune
			if (length(prune[[2]] != 0)) {
				cm <- cm[-unlist(prune[[2]])]
				err <- err[-unlist(prune[[2]])]
			}
		}
		else {
			prune <- active_learning(X=X, y=y_unlabeled, 
				almethod="qbc_prune", n = 0, 
				index=next_sample[[1]],
				committee_pred=next_sample[[2]], 
				k = i, err = err, is_prune = FALSE, 
				...)
			err <- prune[[1]]
		}
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx], committee = cm)
		res[i] <- return_method(tout, X, y, committee = cm)
	}
	# Everything else (not QBC)
	else {
        if (i != 1 & al_method == "us") {
	        # classifier_method re-trains random forest after the oracle
	        # Save computation time by passing those results to US algo
	        # Of course, this only works since classifier = "rf", and the
	        # classifier_method function also uses "rf"
	        next_sample <- active_learning(X,
		        y_unlabeled, al_method, n, 
		        isR = TRUE, tout = tout, ...)
        } else {
	        next_sample <- active_learning(X, y_unlabeled, al_method, n, ...)
        }
 		y_unlabeled[next_sample] <- y[next_sample]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx])
		res[i] <- return_method(tout, X, y)
		}
	}
	res
}
\end{lstlisting}
}

The AL engine without committee pruning (\texttt{AL\_engine\_noprune}) is the 
exact same as \texttt{AL\_engine} with the committee pruning section removed. 
\texttt{AL\_engine\_noprune} is called by the two QBC methods which do not 
utilize committee pruning (see Table~\ref{tab:al:simulations} where 
\textit{C\_Pruning} = F). For the sake of completeness, the code is also 
included below.

{\setstretch{1.0}
\begin{lstlisting}

AL_engine_noprune <- function(X, y, y_unlabeled, al_method,
classifier_method, return_method, iter, n, ...){
	
	stopifnot(nrow(X) == length(y), is.matrix(X), is.factor(y), 
	length(levels(y)) == 2)
	idx <- which(is.na(y_unlabeled))
	stopifnot(length(idx) > 0, all(y[-idx] == y_unlabeled[-idx]), 
	length(y)==length(y_unlabeled),is.factor(y_unlabeled))
	
	res <- rep(0,iter)
	
	### SET THE COMMITTEE HERE
	cm <- c("rf","nb","pls","svmRadialWeights")
	err<- rep(0,length(cm))
	
	for(i in 1:iter){
		# If QBC, the procedure is a little different....
		if (al_method == "qbc") {
			if (i != 1 & 
			as.character(substitute(classifier_method))=="qbc_majority") {
				# QBC Majority method re-trains committee after the oracle
				# Save computation time by passing those results to QBC algo
				next_sample <- active_learning(X=X,
					y=y_unlabeled,
					almethod=al_method,n=n, 
					committee = cm, 
					isMajority = TRUE, 
					tout = tout, ...)
			} else {
			next_sample <- active_learning(X=X,
				y=y_unlabeled, almethod=al_method, 
				n=n, committee = cm, ...)
		}
		y_unlabeled[next_sample[[1]]] <- y[next_sample[[1]]]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx], committee = cm)
		res[i] <- return_method(tout, X, y, committee = cm)
	}
	# Everything else (not QBC, not US)
	else {
		if (i != 1 & al_method == "us") {
			# classifier_method re-trains random forest after the oracle
			# Save computation time by passing those results to US algo
			# Of course, this only works since classifier = "rf", and the
			# classifier_method function also uses "rf"
			next_sample <- active_learning(X, 
				y_unlabeled, al_method, n, 
				isR = TRUE, tout = tout, ...)
		} else {
		next_sample <- active_learning(X, y_unlabeled, al_method, n, ...)
		}
		y_unlabeled[next_sample] <- y[next_sample]
		
		# Compute residual error
		idx <- which(!is.na(y_unlabeled))
		tout <- classifier_method(X[idx,], y_unlabeled[idx])
		res[i] <- return_method(tout, X, y)
		}
	}
	res
}
\end{lstlisting}
}

\subsection{AL algorithm engine}
\label{sec:appendicies:al:simulations:alengine}
{\setstretch{1.0}
\begin{lstlisting}
#' Main active learning engine
#'
#' The missing labels in y are denoted by NA.
#' This method takes X as a matrix of all the data
#'
#' @param X the full data matrix, n x d, including all unlabeled data
#' @param y a factor vector with 2 levels and NAs for unlabeled data
#' @param almethod the AL method name
#' @param n is the number of unlabeled points to be "pooled"
#' @param ... additional parameters for the active learning method
#'
#' @return an index corresponding to the row of X to learn the label of next
#' @export

active_learning <- function(X, y, almethod = "us", n, ...){

	stopifnot(nrow(X) == length(y), is.matrix(X), any(is.na(y)),
	is.factor(y), length(levels(y)) == 2)
	
	if (n == 0) {
		unlabel_index_c <- which(is.na(y))
	} else unlabel_index_c <- sample(which(is.na(y)), n)
	
	switch(almethod,
		us=uncertainty_sample(X,y,unlabel_index_c, ...),
		rs=random_sample(unlabel_index_c, ns = 1, ...),
		qbc=qbc_sample(X,y,unlabel_index_c, ...),
		qbb=qbb_sample(X,y,unlabel_index_c,...),
		qbc_prune=qbc_prune(X=X, y=y, ...),
		cluster=cluster_sample(X,y,unlabel_index_c, ...)
		)
}
\end{lstlisting}
}

\subsection{Simulator (Main)}
\label{sec:appendicies:al:simulations:results}

Finally, the main simulator is what calls the simulation engine 25 times 
(trials) for each active learning method with the parameters described in 
Table~\ref{tab:al:simulations} and collects the results.

{\setstretch{1.0}
\begin{lstlisting}
setwd("---simulation file path---")
# NOTE: AL_header.R loads the simulation R package
# External installation via devtools::install_github("amytian789/thesis-al")
# Local installation via devtools::load_all("---package path---")
source("main/AL_header.R")
source("main/AL_engine.R")
source("main/AL_engine_noprune.R")
source("main/AL_data.R")




################################## Overall classifier and return methods

# MAIN class.model that will train on the data once AL selection is completed
# X and y contain labeled points
classifier_method <- function(X, y, ...) {
	caret::train(X,y,method="rf")
}

# MAIN prediction method for the data once AL selection is completed
# X contains all points to predict
classifier_predict <- function(classifier, X, ...) {
	stats::predict(classifier, X)
}

# Majority Committee Vote classification model
# X and y contain labeled points
qbc_majority <- function(X, y, committee, ...) {
	tout <- vector("list",length(committee))
	for (i in 1:length(committee)){
		tout[[i]] <- caret::train(X,y,committee[i])
	}
	tout
}

# Generic error ratio
# X contain all points. y are known labels (unknown to the learning algorithm)
return_method <- function(classifier, X, y, ...) {
	p <- stats::predict(classifier, X)
	length(which(p != y))/length(y)
}

# QBC error ratio
# X contain all points. y are known labels (unknown to the learning algorithm)
qbc_m_return <- function(tout, X, y, committee, ...) {
	p <- vector("list",length(committee))
	for (i in 1:length(committee)) {
		p[[i]] <- predict(tout[[i]],newdata=X)
	}
	# Aggregate prediction
	ap <- rep(0,length(y))
	for (i in 1:length(y)){
		temp <- as.numeric(as.character(p[[1]][i]))
		for (j in 1:length(committee)){
			temp <- c(temp,as.numeric(as.character(p[[j]][i])))
		}
		# error checking if a value doesn't appear at all
		if (is.na(as.numeric(sort(table(temp),decreasing=TRUE)[2]))) {
			ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
		} else {
			# pick one at random if there is a tie
			if (as.numeric(sort(table(temp),decreasing=TRUE)[1]) ==
			as.numeric(sort(table(temp),decreasing=TRUE)[2])){
				temp <- c(0,1)
				ap[i] <- sample(temp,1)
			} else {
				# Otherwise, insert the first one
				ap[i] <- as.numeric(names(sort(table(temp),decreasing=TRUE)[1]))
			}
		}
	}
	length(which(ap != y))/length(y)
}




################################ Set up the data

load_mnist()
names(train)

# Randomly select the dataset. a and b are the labels which we want to compare 
# (a,b in [0,10]. We are only interested bivariate classification)
a <- 7
b <- 9
n <- 250 # desired dataset size
init <- 10 # desired number of points to initialize with
set.seed(10)
idx <- c(sample(which(train$y == a),n/2),sample(which(train$y == b),n/2))
X <- train$x[idx,]
X <- t(apply(X,1,compressImg)) # compress from 28x28 to 14x14 pixels
y <- as.factor(train$y[idx]) # y contains the "true" labels. y is never seen by 
the AL algorithms 

# Randomly select the initial points given to the AL algorithms
y_unlabeled <- y
set.seed(10)
y_unlabeled[sample(1:n,n-init)] <- NA

# Visual representation of the data
plotTable(13,20,y,X)

rm(train)




###################################

s <- 15 # Number of random unlabeled points to "pool"
	# n = 0 indicates that the "pool" should sample from all data points
k <- 25 # Number of simulations to run
iter <- 50  # Number of AL algorithm iterations (the "budget")




# Classifier performance given all data is labeled
# pred <- classifier_method(X,y)
# perf_results <- rep(return_method(pred,X,y),iter)
# This has been shown to yield perfect results, so it is commented out

# Uncertainty Sampling
us_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	us_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "us", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter, n = s, classifier = "rf")
	print(c("Trial ",i,"complete"))
}

# Query by Committee with "Majority Committee Vote" model
qbc_majority_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine
	qbc_majority_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = qbc_majority,
		return_method = qbc_m_return, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Committee with "Majority Committee Vote" model
# no pruning
qbc_majority_noprune_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine_noprune
	qbc_majority_noprune_results[i,] <- AL_engine_noprune(X=X,
		y=y, y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = qbc_majority, 
		return_method = qbc_m_return, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Committee with overall "Random Forest" model
qbc_rf_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine
	qbc_rf_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Committee with overall "Random Forest" model
# no pruning
qbc_rf_noprune_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	### To change the committee, you must set it in the AL_engine_noprune
	qbc_rf_noprune_results[i,] <- AL_engine_noprune(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbc", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter, n = s, dis = "vote_entropy", pt = 0.5)
	print(c("Trial ",i,"complete"))
}

# Query by Bagging
qbb_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	qbb_results[i,] <- AL_engine(X=X, y=y, 
		y_unlabeled=y_unlabeled, al_method = "qbb", 
		classifier_method = classifier_method,
		return_method = return_method, 
		iter = iter,n = s,classifier="rf", 
		dis = "vote_entropy", num_class=5, r=0.75)
	print(c("Trial ",i,"complete"))
}

# Min-Max Clustering
cluster_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	cluster_results[i,] <- AL_engine(X=X, y=y,
		y_unlabeled=y_unlabeled, al_method = "cluster", 
		classifier_method = classifier_method, 
		return_method = return_method, 
		iter = iter,n = s, dis = "euclidean")
	print(c("Trial ",i,"complete"))
}

# Random Sampling
random_results <- matrix(0,nrow=k,ncol=iter)
for (i in 1:k){
	set.seed(i)
	random_results[i,] <- AL_engine(X, y, y_unlabeled, 
		al_method = "rs", classifier_method, return_method, 
		iter, s)
	print(c("Trial ",i,"complete"))
}

# Average
us_vec <- apply(us_results,2,mean)
random_vec <- apply(random_results,2,mean)
qbc_majority_vec <- apply(qbc_majority_results,2,mean)
qbc_majority_noprune_vec <- apply(qbc_majority_noprune_results,2,mean)
qbc_rf_vec <- apply(qbc_rf_results,2,mean)
qbc_rf_noprune_vec <- apply(qbc_rf_noprune_results,2,mean)
qbb_vec <- apply(qbb_results,2,mean)
cluster_vec <- apply(cluster_results,2,mean)

# Select best QBC output (with pruning)
if (length(which(qbc_majority_vec < qbc_rf_vec)) > 
length(which(qbc_majority_vec > qbc_rf_vec))){
	qbc_prune_vec <- qbc_majority_vec
} else if (length(which(qbc_majority_vec < qbc_rf_vec)) < 
length(which(qbc_majority_vec > qbc_rf_vec))){
	qbc_prune_vec <- qbc_rf_vec
} else{
	# select one at random
	set.seed(1)
	rr <- sample(c(0,1),1)
	if (rr == 0) qbc_prune_vec <- qbc_majority_vec
	else qbc_prune_vec <- qbc_rf_vec
}
# Select best QBC output (with no pruning)
if (length(which(qbc_majority_noprune_vec < qbc_rf_noprune_vec)) > 
length(which(qbc_majority_noprune_vec > qbc_rf_noprune_vec))){
	qbc_noprune_vec <- qbc_majority_noprune_vec
} else if (length(which(qbc_majority_noprune_vec < qbc_rf_noprune_vec)) < 
length(which(qbc_majority_noprune_vec > qbc_rf_noprune_vec))){
	qbc_noprune_vec <- qbc_rf_noprune_vec
} else{
	# select one at random
	set.seed(2)
	rr <- sample(c(0,1),1)
	if (rr == 0) qbc_noprune_vec <- qbc_majority_noprune_vec
	else qbc_noprune_vec <- qbc_rf_noprune_vec
}
# Select best overall QBC output
if (length(which(qbc_prune_vec < qbc_noprune_vec)) > 
length(which(qbc_prune_vec > qbc_noprune_vec))){
	qbc_vec <- qbc_prune_vec
} else if (length(which(qbc_prune_vec < qbc_noprune_vec)) < 
length(which(qbc_prune_vec > qbc_noprune_vec))){
	qbc_vec <- qbc_noprune_vec
} else{
	# select one at random
	set.seed(3)
	rr <- sample(c(0,1),1)
	if (rr == 0) qbc_vec <- qbc_prune_vec
	else qbc_vec <- qbc_noprune_vec
}


################################### Plot the results

date <- Sys.Date()
pdf(file=paste0("results/results_", date, ".PDF"), 
height = 6, width = 10)

### Plot all AL performance
ymax <- max(c(us_vec, random_vec, qbc_vec,cluster_vec))
graphics::plot(1:iter, qbc_vec, ylim=c(0,ymax), lwd=2, type="l", 
	main="Various Active Learning Error Ratios with Random Forest Classifier", 
	xlab="Iterations", ylab="Error", col = "green")
graphics::lines(1:iter, random_vec, lwd = 2, col = "red")
graphics::lines(1:iter, us_vec, lwd = 2, col = "black")
graphics::lines(1:iter, qbb_vec, lwd = 2, col = "blue")
graphics::lines(1:iter, cluster_vec, lwd = 2, col = "orange")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,
	title="Active Learning method",
	legend = c("Random Sampling","Uncertainty Sampling",
	"Query by Committee (best)","Query by Bagging","Min-Max Clustering"),
	col=c("red","black","green","blue","orange"))

### Plot QBC performance
graphics::plot(1:iter,qbc_majority_vec,ylim=c(0,ymax),lwd=2,type="l"
	,main="Query by Committee AL Error Ratio with Various Classifiers", 
	xlab="Iterations", ylab="Error", col = "red")
graphics::lines(1:iter, qbc_majority_noprune_vec, lwd = 2, lty = 2, col = "red")
graphics::lines(1:iter, qbc_rf_vec, lwd = 2, col = "blue")
graphics::lines(1:iter,qbc_rf_noprune_vec, lwd = 2, lty = 2, col = "blue")
graphics::legend(x="bottomleft",lwd=2,cex = 0.75,
	title="Main Classifier | Committee Pruning?",
	legend=c("Majority Committee Vote | Yes", "Majority Committee Vote | No",
	"Random Forest | Yes", "Random Forest | No"),
	col=c("red","red","blue","blue"), lty=c(1,2,1,2))

### Plot 95% confidence intervals

par(mfrow=c(3,2))

# Random Sampling
graphics::plot()
graphics::lines()

# Uncertainty Sampling
graphics::plot()
graphics::lines()

# Min-Max Clustering
graphics::plot()
graphics::lines()

# Query by Bagging
graphics::plot()
graphics::lines()

# Query by Committee (best)
graphics::plot()
graphics::lines()

graphics.off()
save.image(file = paste0("results/results_", date, ".RData"))
\end{lstlisting}
}