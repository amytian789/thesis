\section{Overview of graph summarization methods}
\label{sec:gc:methods}

There are many different ways to summarize a graph. However, akin to how a
correlation coefficient cannot sufficiently capture all aspects of dependency 
on its own, there is no ``best'' summarization method that can summarize 
every quality of a graph on its own. As such, this section discusses various 
graph summarization methods, the qualities that they measure, and their 
drawbacks. 
In section~\ref{sec:gc:examples}, we propose a method to select graph 
$\hat{G^i}, i \in \{1,...,k\}$ where $\hat{G^i}$ is most similar to the 
``base'' graph $\hat{G}$. $\hat{G^i}$ is selected out of $k$ given graphs each 
associated with a vector $\overrightarrow{d^{i,j}}$ where $i\neq j$ and $i,j 
\in \{1,...,k\}$ of their differences with each difference 
$\overrightarrow{d^{i,j}_m}$ for $m \in \{1,...,6\}$ corresponding to one of 
the 6 summarization methods listed below.

Most methods listed below are implemented in the \texttt{igraph} package in 
\texttt{R}. As such, we do not implement most methods ourselves as in 
Chapter~\ref{ch:al}. However, we do need to normalize the returned 
summarization metric in order to compute the difference properly. The code for 
all methods may be found in Appendix~\ref{sec:appendicies:gc:engine}.
For all methods below, let $\hat{G}(V,E)$ be some labeled graph.

\subsection{Centrality}

The centrality of a graph $\hat{G}$ is given by $\mathcal{C}$ and measures 
of the importance of its 
nodes. Let $\overrightarrow{c}$ be a vector of length $V$ where each element 
$\overrightarrow{c_i}$ is the ``importance'' of node $i$. Then the centrality 
of $\hat{G}$ is given by
$$c = \sum\limits^{len(V)}_{i=1} \max_{\forall j \in \{1,...,len(V)\}} 
\overrightarrow{c_j} - \overrightarrow{c_i}$$
The importance of the node may be measured by utilizing metrics such as 

\tablespacing
\begin{itemize}
	\item \textbf{Degree:} The degree of a node is the number of edges the node 
	is part of. In other words, it is the number of direct paths from the node 
	to all other nodes. The higher the degree, the more important the node 
	is.
	\item \textbf{Closeness:} The closeness of a node is the average 
	shortest path length (the path which traverses the smallest number of 
	edges) between the node and all other nodes. The higher the closeness, 
	the closer the node is to all other nodes (in terms of path length), the 
	more important the node is.
	\item \textbf{Betweenness:} The betweenness of a node is the number of 
	times the node is part of the shortest path between two other nodes. Direct 
	paths are excluded. The higher the betweenness, the more important the node 
	is as it provides a critical cost-saving link (path length) for the path 
	between many other nodes. 
\end{itemize}
\bodyspacing

The normalized centrality of a graph $\hat{G}$ may be computed as 	
$\frac{Cen(\hat{G})}{\max Cen(\hat{G})}$ where $\max Cen(\hat{G})$ is the 
maximum centrality that graph $\hat{G}$ may have (for whatever metric is 
specified). Degree centrality is implemented in \texttt{centr\_degree}, 
closeness centrality is implemented in \texttt{centr\_clo}, and betweenness 
centrality is implemented in \texttt{centr\_betw} from the \texttt{igraph} 
package. By default, each function normalizes $\mathcal{C}$.

Drawbacks = ?

\subsection{Assortativity}

Assortativity measures the level of resemblance among the connected nodes of a 
graph $\hat{G}$ based on vertex category labels. For graphs with uncategorized 
nodes (as in the examples of Section~\ref{sec:gc:examples} and the application 
in Chapter~\ref{ch:usage}), it is common to use $deg(V_i)-1$
as the category label for node $V_i$ instead. For 
an edge $E_{i,j}$, the degree of nodes $V_i$ and $V_j$ are collected in two 
distinct vectors $\overrightarrow{v^1}, \overrightarrow{v^2}$. Then the 
assortativity metric of a graph $\hat{G}$ is $\mathcal{A} = 
\rho(\overrightarrow{v^1},\overrightarrow{v^2})$ where $\rho$ is the Pearson 
correlation as defined in Section~\ref{sec:intro:correlation}. As such, 
the assortativity of $\hat{G}$ is summarized in a single value 
$-1 \leq \mathcal{A} \leq 1$ where the interpretation is the same as that of 
the Pearson correlation coefficient. For instance, $\mathcal{A}=1$ when nodes 
with high/low degrees in $\hat{G}$ are mostly connected to other nodes with 
high/low degrees in $\hat{G}$, respectively. In other words, if 
$\mathcal{A}$ is large, then connected vertices tend to have the same 
qualities, and when $\mathcal{A}$ is small, then connected vertices tend to 
have opposite qualities. Since assortativity involves a correlation 
computation, the result is already standardized.
Assortativity is called with the \texttt{assortativity\_degree} function from 
the \texttt{igraph} package.

Drawbacks = ?

\subsection{Community}

The community of a graph $\hat{G}$ is given by $\mathcal{S}$, a set of its 
dense subgraphs. Subgraphs may be detected with any of the following algorithms:

\tablespacing
\begin{itemize}
	\item \textbf{Infomap:} 
	This approach to building a community is called with the 
	\texttt{cluster\_infomap} function from the \texttt{igraph} package.
	\item \textbf{Random walks:} By implementing a random walk within a graph 
	to determine the various subgraphs, the random walk procedure takes 
	advantage of the natural idea that random walks within a graph will tend to 
	stay in the same subgraph. This is because a subgraph is a dense part of 
	the graph with few connections to other subgraphs, so there is an 
	increased likelihood of proceeding via an edge that is within the same 
	subgraph.
	This approach to building a community is called with the 
	\texttt{cluster\_walktrap} function from the \texttt{igraph} package.
\end{itemize}
\bodyspacing

There is no real way or need to normalize $\mathcal{S}$, the the returned set 
of subgraphs. What is important to note, however, is that comparison among 
graphs $\hat{G^1}(V^1,E^2)$ and $\hat{G^2}(V^2,E^2)$ via the community graph 
summarization metric 
must compare $\mathcal{S}^1$ and $\mathcal{S}^2$ with a method such as the 
Jaccard index. Consider the sets $\mathcal{S}^1=(1,1,2,2,3,3)$ and 
$\mathcal{S}^2=(3,3,2,2,1,1)$. These are 
the exact same sets, but they are numbered differently; it is clear, then, that 
a traditional distance metric such as Euclidean distance will not capture the 
relationship properly. The Jaccard index compares the actual sets rather than 
their literal values.

Drawbacks = ?

\subsection{Distance matrix}

The distance matrix $M$ of a graph $\hat{G}$ is a square matrix that 
contains the shortest path length 
between all pairs of variables. Each element $M_{i,j}$ 
contains the shortest path distance between node $i$ to node $j$. If no path 
exists, it is conventional to set the value to infinity. However, since the 
main purpose is to compare two matrices, 
setting a value of infinity doesn't make sense. Alternatively, the value may be 
set to zero since we define the shortest path distance earlier to be 
\textit{the path which traverses the smallest number of edges}, so a 
\textit{direct path} between nodes has path length 1. Thus, the shortest path 
length can never be 0, which allows the existence of a ``non-existent path'' to 
be distinguished from the ``length of an existing path''. The metric may be 
normalized by setting $M_{i,j} = \frac{\min len(V_i,V_j)}{V-1}$ for all node 
pairs $(i,j)$ since the longest possible shortest path for any pair 
$(V_i,V_j)$ must traverse all other nodes before arriving at $V_j$, thereby 
bounding the value.
The distance matrix method is called with the \texttt{distances} function from 
the \texttt{igraph} package. \texttt{distances} returns infinity when a path 
doesn't exist, so the output is adjusted accordingly.

Drawbacks: add edge between 2 isolated clusters

\subsection{Edge connectivity}

The edge connectivity of a graph $\hat{G}$ is given by $\mathcal{E}$, the 
number of edges that need to be deleted in order to \textit{disconnect} 
$\hat{G}$ such that there exists two nodes $V_i,V_j$ in $\hat{G}$ which do not 
have a path between them.
%More specifically, it is the number of edges that need to be 
%deleted so that $\hat{G}$ is partitioned into $k$ distinct subgraphs and is
%no longer ``strongly'' connected. 
As the metric is upper bounded by the minimum 
degree of any node within a graph, the metric may be normalized by computing 
$\frac{\mathcal{E}}{\min_i deg(V_i)}$ for all possible nodes $i$.
The edge connectivity of a graph is computed 
with the \texttt{edge\_connectivity} function from the \texttt{igraph} package. 

Drawbacks: rotated edges within the graph

\subsection{Edge density histogram}

Edge density captures the number of nodes in a graph $\hat{G}$ that have a 
specified number of degrees. Naturally, a larger number of high-degree nodes is 
a signal for density while a larger number of low-degree nodes is a signal for 
sparsity. For $\hat{G}$, the edge density is summarized in a 
vector $\overrightarrow{v}$ of length $b$, the number of desired ``bins'' 
(e.g. bin 1 is $1-5$ degrees, bin 2 is $6-10$ degrees, etc.). Then the value 
$\overrightarrow{v_i}$ is the number of nodes which correspond to bin $i$. This 
metric may be normalized by computing $\frac{\overrightarrow{v_k}}{V}$ for 
all $k \in \{1,...,b\}$ since there are a total of $V$ nodes. 
This summarization method is not implemented in the \texttt{igraph} package. 

Drawbacks: rotated edges within the graph