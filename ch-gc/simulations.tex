\section{Similarity selection}
\label{sec:gc:simulations}

Given a set of $k$ graphs $\hat{G^i}$ for $i \in \{1,...,k\}$, we would like to 
select the graph $\hat{G^*}$ which is most similar to the ``base'' graph 
$\hat{G}$. Each of the $k$ given graphs are associated with a vector 
$d^{i}$ of their difference to $\hat{G}$. Each element $d^{i}_m$ for $m \in 
\{1,...,10\}$ corresponds to one of the ten summarization methods listed 
earlier in Section~\ref{sec:gc:methods} (different flavors of 
centrality and community are included). 
We propose the following methodology to select the most similar graph pair:

\tablespacing
\begin{enumerate}
	\item Compare each difference metric $d^{i}_m$ against 
	$d^{i'}_m$ for all $m \in \{1,...,6\}$ and $i,i' \in 
	\{1,...,k\}$ where $i \neq i'$. 
	\item Select the index $i$ which has the most number of smallest 
	differences. In other words, find
	$$\max\limits_{i \in \{1,...,k\}} 
	\sum\limits^{10}_{m=1} \textbf{1}_{d^{i}_m > d^{i'}_m} \ \text{where} \ i 
	\neq i' \in \{1,...,k\}$$
\end{enumerate}
\bodyspacing

Aside from returning the most similar graph pair, the proposed similarity 
selection method further eliminates the need to normalize the resulting 
difference metrics (which will not be on the same scale within the vector
$d^i$), as it compares each metric to its own metric rather 
than using $d^i$ to compute a scalar ``difference'' between graphs 
$\hat{G}^i$ and $\hat{G}$. In the next section, we demonstrate the viability of 
this strategy with a simulation study. This method is later applied in 
Chapter~\ref{ch:usage} to compare the visual correlation graph (formed from the 
VS) with the various numerical correlation graphs (formed from the correlation 
coefficients described in Section~\ref{sec:intro:correlation}).
The similarity selection method is coded in 
Appendix~\ref{sec:appendicies:gc:similarity}.

\subsection{Simulation study}
\label{sec:gc:simulations:algorithm}

We now perform a simulation study to demonstrate the viability of the proposed 
selection strategy. Let $G=(V,E)$ be a base graph with $|V|=20$ nodes and 
$|E|=50$ edges. The idea is to generate two random graphs $G^1, G^2$ from $G$ 
such that $\text{diff}(G,G^1) << \text{diff}(G,G^2)$ or vice versa. 
In other words, one graph should clearly be similar to $G$ while the other is 
very dissimilar to $G$. This is achieved by randomly swapping $\frac{|E|}{5} = 
10$ and $|E| = 50$ edges from $G$ to build $G^1$ and $G^2$, respectively. 
The random swapping algorithm is as follows:

\tablespacing
\begin{algorithm}[H]
	\caption{Random edge swapping}\label{alg:gc:simulations:swapping}
	\begin{algorithmic}[1]
		\Procedure{}{$G=(V,E)$ is the ``base'' graph. Generate a 
		random graph $G^i=(V,E^i)$ by randomly swapping $e$ edges of $G$.
		Let $p_k$ be the probability of randomly selecting node $V_k$. }
		\State $G^i \gets G$
		\State \textbf{loop from} $f=1$ \textbf{to} $e$:
		\State \indent Select an edge uniformly at random and delete from $G^i$
		\State \indent Select $V_i$ uniformly at random
		\State \indent Select $V_j$ at random where 
		$p_k = \frac{\text{deg}(V_k)}{\sum\limits_{x=1}^{|V|} \text{deg}(V_x)} 
		\ \forall \
		k \in \{1,...,|V|\}$
		\State \indent \textbf{while} ($V_i == V_j$ \textbf{or} edge 
		$(V_i,V_j)$ exists):
		\State \indent \indent Reselect $V_j$ at random with the PDF given 
		earlier*
		\State \indent Add edge $(V_i,V_j)$ to $G^i$
		\State \textbf{return} where($p==\min{(p)}$)
		\EndProcedure
	\end{algorithmic}
	*Selecting the second node in this manner ensures that the assortativity 
	coefficient continues to evolve as the swapping proceeds. 
\end{algorithm}
\bodyspacing

\noindent Naturally, $G^1$ is more similar to $G$ since less 
edges are being swapped than in $G^2$. The differences between the pair 
$(G,G^i)$ for $i \in \{1,2\}$ are computed with the summarization 
methods discussed in Section~\ref{sec:gc:methods}, and the most similar 
graph is selected with the similarity selection method described in 
Section~\ref{sec:gc:simulations}. 
The results are recorded for 1000 trials, and an average is computed. 
The code for the simulation study may be found in 
Appendix~\ref{sec:appendicies:gc:simulations}. 

\subsection{Results}
\label{sec:gc:simulations:results}

The results are summarized in Table~\ref{tab:gc:simulations}.

\tablespacing
\begin{longtable}{p{0.15\linewidth} p{0.25\linewidth} 
p{0.15\linewidth} p{0.15\linewidth}}
	
	% First page heading
	\caption[Summary of similarity selection simulation results.]{A summary of 
	the  similarity selection simulation results as described in 
	Section~\ref{sec:gc:simulations:algorithm}. The table summarizes the 
	probability of selecting graphs $G^1$ and $G^2$ with the given number of 
	swaps. (C) is an abbreviation for (CONTROL)} 
	\label{tab:gc:simulations}\\
	\toprule
	\textbf{Distance} & \textbf{Number of swaps} & 
	\textbf{Prob}($G^1$) & \textbf{Prob}($G^2$) \\
	\midrule
	\endfirsthead
	
	% Future page heading
	\caption[]{(continued)}\\
	\toprule
	\textbf{Distance function} & \textbf{Number of swaps} & 
	\textbf{Prob}($G^1$) & \textbf{Prob}($G^2$) \\
	\midrule
	\endhead
	
	% Page footer
	\midrule
	\multicolumn{4}{r}{(Continued on next page)}\\
	\endfoot
	
	% Last page footer
	\bottomrule
	\endlastfoot
	
	Euclidean & $G^1: 10, G^2: 10$ (C) & 
	0.54 & 0.46 \\
	
	& $G^1:10, G^2: 50$ & 0.885 & 0.115 \\
	
	\cmidrule[0.1pt](l{0.5em}r{0.5em}){1-4}	
	
	L1 & $G^1: 10, G^2: 10$ (C) & 
	0.532 & 0.468 \\
	
	& $G^1:10, G^2: 50$ & 0.886 & 0.114 \\
	
	\cmidrule[0.1pt](l{0.5em}r{0.5em}){1-4}	

	L2 & $G^1: 10, G^2: 10$ (C) & 
	0.541 & 0.459 \\
	
	& $G^1:10, G^2: 50$ & 0.885 & 0.115 \\
		
\end{longtable}
\bodyspacing

\noindent Though the control is slightly skewed towards $G^1$, it is still 
clear that, on average, the proposed similarity selection method is more likely 
to properly select $G^1$, the actual ``most similar graph''. To visualize 
the differences between 10 vs 10 and 10 vs 50 swaps, please review 
Figures~\ref{fig:gc:sim10vs10} and~\ref{fig:gc:sim10vs50}, which plot $G, G^1,$ 
and $G^2$ from the final simulation trial. Because the control and the regular
simulations were run separately and because each trial is seeded with 
its trial number, $G$ and $G^1$ (which has 10 swaps in each 
simulation) are the same in Figures~\ref{fig:gc:sim10vs10} 
and~\ref{fig:gc:sim10vs50}; only $G^2$ differs.

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=1\linewidth]{ch-gc/figures/sim_10vs10}
		\caption[Random graphs with 20 nodes and 50 edges. Edges have been 
		swapped 10 times each.]{\textit{Left:} Base graph $G=(V,E)$ with 
		$|V|=20$ nodes and $|E|=50$ edges. 
		\textit{Middle:} Random graph $G^1$ built from $G$ by swapping 
		10 edges. \textit{Right:} Random graph $G^2$ built from $G$ by 
		swapping 10 edges. Distance: L2. Seed: 1000}
		\label{fig:gc:sim10vs10}
	\end{center}
\end{figure}

\begin{figure}[htb]
	\begin{center}
		\includegraphics[width=1\linewidth]{ch-gc/figures/sim_10vs50}
		\caption[Random graphs with 20 nodes and 50 edges. Edges have been 
		swapped 10 and 50 times, respectively.]{\textit{Left:} Base graph 
		$G=(V,E)$ with $|V|=20$ nodes and $|E|=50$ edges.\textit{Middle:} 
		Random graph $G^1$ built from $G$ by swapping 10 edges. 
		\textit{Right:} Random graph $G^2$ built from $G$ by swapping 
		50 edges. Distance: L2. Seed: 1000}
		\label{fig:gc:sim10vs50}
	\end{center}
\end{figure}

\subsubsection{Extensions}

Although the normalization methods proposed in Section~\ref{sec:gc:methods} 
have bounded the individual graph summarization metrics between 0 and 1, these 
metrics are not necessarily uniformly distributed between 0 and 1. An 
alternative method of normalization utilizes the Erdos-Renyi model (which 
generates a random graph by creating every edge with the same probability) to 
normalize the \textit{overall difference metric} between 
two existing graphs $G^1=(V,E^1)$ and $G^2=(V,E^2)$. While this does not 
actually yield a scalar difference metric, the result provides insight on 
the significance of the resulting difference metric.
Let $d = \text{diff}(G^1,G^2)$. Then the procedure is as follows:

\tablespacing
\begin{algorithm}[H]
	\caption{Alternative graph difference 
	normalization procedure}\label{alg:gc:simulations:extension}
	\begin{algorithmic}[1]
		\Procedure{}{$G^1=(V,E^1)$ and $G^2=(V,E^2)$. Let $d = 
		\text{diff}(G^1,G^2)$. $n$ is the desired number 
		of trials e.g. $n = 1000$. }
		\State \textbf{loop from} $i=1$ \textbf{to} $n$:
		\State \indent Generate an Erdos-Renyi graph $\hat{G^1}$ with 
		$|V|$ nodes and $|E^1|$ edges.
		\State \indent Generate an Erdos-Renyi graph $\hat{G^2}$ with 
		$|V|$ nodes and $|E^2|$ edges.
		\State \indent $t_i \gets \text{dist}(\hat{G^1},\hat{G^2})$
		\State Compare $d$ to the empirical distribution $\{t_1,...,t_n\}$*
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\bodyspacing
 
\noindent *Compare $d$ to the empirical distribution $\{t_1,...,t_n\}$ by 
computing a $p$-value
$$\frac{\#\{t_i : t_i\geq d \}}{n}$$

\noindent for an upper tail distribution and a $p$-value
$$\frac{\#\{t_i : t_i \leq d \}}{n}$$

\noindent for a lower tail distribution. The $p$-values may be computed using 
the similarity selection methodology 
presented in Section~\ref{sec:gc:simulations}. A lower $p$-value 
suggests that $d$ is more significant (i.e. the computed difference metric is 
reliable) because it indicates that $d$ has a low probability of being an 
``extreme'' event.